\documentclass[a4paper,12pt,twoside,BCOR=10mm]{scrbook}

% IMPORTANT TODO :: go through all TODOs and especially all IMPORTANT TODOs!
%
% for citation commands, see:
% http://merkel.zoneo.net/Latex/natbib.php
%
% Here are the ones that we use; first the two main ones:
% \citep{jon90}               --> (Jones et al., 1990)
% \citep[see][]{jon90}        --> (see Jones et al., 1990)
%
% But we also have for a specific part (chapter, page, ...) that is cited:
% \citep[chap. 2]{jon90}      --> (Jones et al., 1990, chap. 2)
% \citep[see][chap. 2]{jon90} --> (see Jones et al., 1990, chap. 2)
% see \citet[chap. 2]{jon90}  --> see Jones et al. (1990, chap. 2)
%
% And if we want to cite several at once:
% \citep{jon90,jam91}         --> (Jones et al., 1990; James et al. 1991)
%
% Finally, for in-text citations, we have:
% \citet{jon90}               --> Jones et al. (1990)
%
%
%
% TODO :: search for every occurrence of "\color{red}" or of "(imagine some text here)" and figure that stuff out =)
%
% TODO :: das ganze Dokument durchsuchen und "Burrows-Wheeler", "Burrows---Wheeler", "Burrows - Wheeler"
%         durch "Burrows--Wheeler" ersetzen sowie "preprocess" durch "pre-process" ersetzen (oder andersrum,
%         aber jedenfalls nicht andauernd hin und her springen ^^), "bit-vector" durch "bit vector" ersetzen
%         und schließlich auch noch "\cite{" durch "\citep{" sowie "Javascript" durch "JavaScript" (CamelCase, yay! ^^)
%
% TODO :: decide whether to use SNPs or snips, and then stick with one ;) (just now I used "snips", but seriously, check that!)
%
% TODO :: replace " - " with "---" (but case-to-case, nicht einfach so blanko!)
%
% TODO :: search for ?? in the PDF document, as that is what missing links will come out as, and link them up proper!
%
% TODO :: alles nochmal auf verb tenses überprüfen, also:
% "Use present tense to express general truths or facts or conclusions supported by research results that are unlikely to change.
% In other words, something that is believed to be always true."
% http://cgi.stanford.edu/~dept-ctl/tomprof/posting.php?ID=1009
%
% TODO :: auch nochmal die algorithm captions durchgehen (am besten im Inhaltsverzeichnis),
% und schauen, ob jeweils das Gerundium mit -ing genutzt wurde, oder nie - beides wäre okay
% - oder ob mal so mal so - was verwerflich wäre!
%
% TODO :: nochmal durchschauen, ob wir die folgenden Regeln alle so richtig eingehalten haben:
%
% alle Texte sollen schwarz sein (mit Ausnahme von Bildern, in denen auch Farben erlaubt sind)
% fett bitte nur spärlich verwenden
% unterstrichene Texte bitte gar nicht verwenden
% ausdrücklich sollen Hyperlinks vermieden werden, die unterstrichen oder in Farbe dargestellt werden!
% man sollte sparsam mit footnotes (also Fußnoten) umgehen
%   wenn man sie einsetzt, dann sollten sie nummeriert sein und möglichst direkt auf der Seite der ersten Erwähnung aufgeführt werden
% man sollte nicht tiefer als subsubsections gehen, also: chapters ok, sections ok, subsections ok, subsubsections ok, mehr nicht =) 
%
%
% zum Druck:
% 
% alles soll in A4 ausgedruckt werden
% natürlich soll beidseitig gedruckt werden, und Kapitel sollen immer auf der rechten Seite starten
% die Druckerei erwartet oft eine PDF-Datei
% (die meist mit höchstmöglichen qualitativen Einstellungen exportiert werden muss und alle Bilder embedded hat)
% Achtung! Farbdrucke sind nicht gerade billig, hier aber echt notwendig. ;)
%
%
% und noch ein paar Dinge:
%
% Title page, spine, and back page
% 
% Logos of sponsors or whoever should not be used; not on the front, the back, or anywhere else either ;)
% Instead, all thanks should go into a chapter "Acknowledgements" or in the preamble. (And even then as text, not logo!)
% There should be no picture on the front.
% 
% On the backside can be the name of the printing press (at most 10 pt Verdana in regular white letters),
% þá miðjað hægri-vinstri á blaðsíðu og miðjað upp-niður innan litaborðans. 
% 
% The colored area should go around the spine and onto the back of the book, and go 7,7 cm up from the bottom of the page.
% 
% The colored area should be orange, the color of the Natural Sciences at UI.
% In particular, the color is: CMYK:  0 : 75 : 100 : 0; Pantone: 158 C; RGB: 236 : 78 : 34.
% 
% On the spine of the book you should have the name of the author, the short title (no more than 50 characters)
% and the year of publication in one line in Verdana.
% (The year should be in white letters on the colorful background, while the name of the author and the title should be
% black on white.)
% 
% Here are two examples / instructions on how to do things right:
% 
% Friðrík H. Jónsson og Sigurður J. Grétarsson (2007). \textit{Gagnfræðikver fyrir háskólanema}, Háskólaútgáfan, Reykjavík.
% Ingibjörg Axelsdóttir og Þórunn Blöndal (2006).  \textit{Handbók um ritun og frágang},  Mál og menning, Reykjavík.
 
% Packages
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[icelandic, english]{babel}
\usepackage{t1enc}
\usepackage{graphicx}
\usepackage[intoc]{nomencl}
\usepackage{enumerate,color}
\usepackage[hyphens]{url}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{appendix}
\usepackage{eso-pic}
\usepackage{amsmath}
\usepackage{amssymb}

% we added this one to get numbered definitions and theorems and proofs and so on... not sure if we should even have them,
% as this is compsci, not maths (and in fact it kind of is bioinformatics, so it is even further removed from nice maths ^^),
% but f it - proofs and definitions are a language we speak, and we MAKE IT SO! =)
\usepackage{amsthm}
% this here should be theoremstyle plain for the theorem, but then the theorem is written in italics, and I really want to
% avoid that as I honestly think it is difficult to read...
% \theoremstyle{definition}
% \newtheorem{moyatheorem}{Theorem}[chapter]
% \theoremstyle{definition}
% \newtheorem{moyadef}{Definition}[chapter]

\usepackage[nottoc,numbib]{tocbibind}
\usepackage[sort&compress,authoryear]{natbib}
%\usepackage[sort&compress,square,numbers]{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\usepackage[sf,normalsize]{subfigure}
\usepackage[format=plain,labelformat=simple,labelsep=colon]{caption}
\usepackage{placeins}
\usepackage{tabularx}
\usepackage{filecontents}

% the following added for using colored cells in tables
\usepackage{colortbl}
% \definecolor{purple_fx}{rgb}{0.66,0,0.66}
% \definecolor{purple_bg}{rgb}{1.0,1.0,0.66}
% \definecolor{red_fx}{rgb}{0.8,0,0}
% \definecolor{red_bg}{rgb}{1.0,0.8,1.0}
\definecolor{purple_fx}{rgb}{1.0,1.0,1.0}
\definecolor{purple_bg}{rgb}{0.66,0,0.66}
\definecolor{red_fx}{rgb}{1.0,1.0,1.0}
\definecolor{red_bg}{rgb}{0.8,0,0}
\definecolor{green_fx}{rgb}{1.0,1.0,1.0}
\definecolor{green_bg}{rgb}{0,0.75,0}
\definecolor{blue_fx}{rgb}{1.0,1.0,1.0}
\definecolor{blue_bg}{rgb}{0,0.5,1.0}
\definecolor{yellow_fx}{rgb}{0,0,0}
\definecolor{yellow_bg}{rgb}{1.0,0.75,0.125}

% following line added based on Pall's email
\usepackage[a4paper,left=3cm,right=2cm,top=2.5cm,bottom=2.0cm, includefoot]{geometry}

% following line added for algorithm environments and to print a list of algorithms;
% see https://en.wikibooks.org/wiki/LaTeX/Algorithms
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}

% make table of contents clickable (in electronic devices, of course... not on paper ^^)
\usepackage{hyperref}

% listing with linebreaks for tests in the appendix
\usepackage{listings}
\lstset{breaklines=true}

% BTW, for example reference types, see https://www.sharelatex.com/learn/Bibliography_management_with_natbib

\begin{filecontents*}{thesisreferences.bib}
%
% ===================================================================================================================
%                                                     REFERENCES
% ===================================================================================================================
%
% TODO :: alle ungenutzten Referenzen noch irgendwo unterbringen
%
% genutzt
@article{Siren2014,
    author =       "Jouni Sirén and Niko Välimäki and Veli Mäkinen",
    title =        "Indexing Graphs for Path Queries with Applications in Genome Research",
    journal =      "IEEE/ACM Transactions on Computational Biology and Bioinformatics",
    volume =       "11",
    number =       "2",
    pages =        "375--388",
    year =         "2014",
    DOI =          "10.1109/TCBB.2013.2297101"
}
%
% genutzt
@book{bioinfoalgorithms,
    author    = "Phillip Compeau and Pavel Pevzner",
    title     = "Bioinformatics Algorithms",
    year      = "2014",
    publisher = "Active Learning Publishers",
    address   = "La Jolla, CA, USA",
	ISBN      = "978-0-9903746-0-2"
}
%
% genutzt
@online{specFASTG,
    author =       "David Jaffe and Iain MacCallum and Daniel Rokhsar and Michael Schatz",
    title =        "The {FASTG} Format Specification",
    version =      "1.00",
    year =         "2012",
	URL =          "http://fastg.sourceforge.net/FASTG_Spec_v1.00.pdf",
	note =         "Accessed: 4th of April 2015"
}
%
% genutzt
@online{specGFA1,
    author =       "Heng Li",
	title =        "A proposal of the {Grapical Fragment Assembly} format",
    year =         "2014",
    URL =          "http://lh3.github.io/2014/07/19/a-proposal-of-the-grapical-fragment-assembly-format/",
	note =         "Accessed: 4th of April 2015"
}
%
% genutzt
@online{specGFA2,
    author =       "Heng Li",
	title =        "First update on {GFA}",
    year =         "2014",
    URL =          "http://lh3.github.io/2014/07/23/first-update-on-gfa/",
	note =         "Accessed: 4th of April 2015"
}
%
% genutzt
@online{specGFA3,
    author =       "Heng Li",
	title =        "On the graphical representation of sequences",
    year =         "2014",
    URL =          "http://lh3.github.io/2014/07/25/on-the-graphical-representation-of-sequences/",
	note =         "Accessed: 5th of April 2015"
}
%
% genutzt
@online{knightGFA1,
    author =       "James Knight",
	title =        "Graphs, Alignments, Variants and Annotations, pt. 1",
    year =         "2014",
    URL =          "http://knightlab.commons.yale.edu/gava-pt-1/",
	note =         "Accessed: 6th of April 2015"
}
%
% NICHT GENUTZT
@online{knightGFA2,
    author =       "James Knight",
	title =        "Graphs, Alignments, Variants and Annotations, pt. 2",
    year =         "2014",
    URL =          "http://knightlab.commons.yale.edu/gava-pt-2/",
	note =         "Accessed: 6th of April 2015"
}
%
% genutzt
@online{specSAM,
    author =       "{The SAM/BAM Format Specification Working Group}",
    title =        "Sequence Alignment/Map Format Specification",
    version =      "ec1fec2",
    year =         "2015",
	URL =          "http://samtools.github.io/hts-specs/SAMv1.pdf",
	note =         "Accessed: 23rd of April 2015"
}
%
% genutzt
% SAM2009 is reference [4] from the grant proposal.
@article{SAM2009,
    author =       "Heng Li and Bob Handsaker and Alec Wysoker and Tim Fennell and Jue Ruan and Nils Homer and Gabor Marth and Goncalo Abecasis and Richard Durbin and 1000 Genome Project Data Processing Subgroup",
    title =        "The Sequence Alignment/Map format and {SAMtools}",
    journal =      "Bioinformatics",
    volume =       "25",
    number =       "16",
    pages =        "2078--2079",
    year =         "2009",
    DOI =          "10.1093/bioinformatics/btp352"
}
%
% UNGENUTZT
% in this paper, BWA is explained
@article{Li2009,
    author =       "Heng Li and Richard Durbin",
    title =        "Fast and accurate short read alignment with {Burrows--Wheeler transform}",
    journal =      "Bioinformatics",
    volume =       "25",
    number =       "14",
    pages =        "1754--1760",
    year =         "2009",
    DOI =          "10.1093/bioinformatics/btp324"
}
%
% genutzt
% in this paper, BWA-MEM is explained
@article{Li2013,
    author =       "Heng Li",
    title =        "Aligning sequence reads, clone sequences and assembly contigs with {BWA-MEM}",
    year =         "2013",
    note =         "arXiv:1303.3997 [q-bio.GN]"
}
%
% genutzt
% Burrows1994 is reference [5] from Siren2014.
% It is the paper in which the BWT was introduced; not having anything to do with bioinformatics, mind you,
% but as a compression algorithm!
@online{Burrows1994,
    author =       "Michael Burrows and David J. Wheeler",
	title =        "A block-sorting lossless data compression algorithm. \textit{Digital Equipment Corporation.} Tech. Rep. 124",
    year =         "1994",
    URL =          "http://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-124.html",
	note =         "Accessed: 7th of April 2015"
}
%
% genutzt
% Schneeberger2009 is reference [12] from Siren2014.
% Gives the much-needed basis for the hash-based approach.
% In general, it is very full of awesome ideas and approaches, both in its intro and outro and everywhere in between. =D
@article{Schneeberger2009,
    author =       "Korbinian Schneeberger and Jörg Hagmann and Stephan Ossowski and Norman Warthmann and Sandra Gesing and Oliver Kohlbacher and Detlef Weigel",
    title =        "Simultaneous alignment of short reads against multiple genomes",
    journal =      "Genome Biology",
    volume =       "10",
    number =       "9",
    pages =        "R98",
    year =         "2009",
    DOI =          "10.1186/gb-2009-10-9-r98"
}
%
% genutzt
% Puglisi2007 is reference [21] from Siren2014.
@article{Puglisi2007,
    author =       "Simon Puglisi and W. F. Smyth and Andrew H. Turpin",
    title =        "A Taxonomy of Suffix Array Construction Algorithms",
    journal =      "ACM Computing Surveys",
    volume =       "39",
    number =       "2",
    pages =        "Article 4",
    year =         "2007",
    DOI =          "10.1145/1242471.1242472"
}
%
% genutzt
% Albers2010 is reference [13] from Siren2014.
@article{Albers2010,
    author =       "Cornelis A. Albers and Gerton Lunter and Daniel G. MacArthur and Gilean McVean and Willem H. Ouwehand and Richard Durbin",
    title =        "Dindel: Accurate indel calls from short-read data",
    journal =      "Genome Research",
    volume =       "21",
    pages =        "961--973",
    year =         "2011",
    DOI =          "10.1101/gr.112326.110"
}
%
% genutzt
% Huang2013 is reference [14] from Siren2014.
@article{Huang2013,
    author =       "Lin Huang and Victoria Popic and Serafim Batzoglou",
    title =        "Short read alignment with populations of genomes",
    journal =      "Bioinformatics",
    volume =       "29",
    number =       "13",
    pages =        "i361--i370",
    year =         "2013",
    DOI =          "10.1093/bioinformatics/btt215"
}
%
% genutzt
@article{Lee2002,
    author =       "Christopher Lee and Catherine Grasso and Mark F. Sharlow",
    title =        "Multiple sequence alignment using partial order graphs",
    journal =      "Bioinformatics",
    volume =       "18",
    number =       "3",
    pages =        "452--464",
    year =         "2002",
    DOI =          "10.1093/bioinformatics/18.3.452"
}
%
% genutzt
@article{Holt2014,
    author =       "James Holt and Leonard McMillan",
    title =        "Constructing {Burrows--Wheeler} Transforms of Large String Collections via Merging",
    journal =      "BCB '14, Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics",
    pages =        "464--471",
    year =         "2014",
    DOI =          "10.1145/2649387.2649431"
}
%
% genutzt
% Myers 2005 seems to be mostly about DNA string graphs (yay!) used when building up a NEW unknown reference 
% from reads only, not for aligning reads to an already existing reference...
% But still, it looks like it can be helpful and citeable! =)
@article{Myers2005,
    author =       "Eugene W. Myers",
    title =        "The fragment assembly string graph",
    journal =      "Bioinformatics",
    volume =       "21",
    number =       "Suppl. 2",
    pages =        "ii79--ii85",
    year =         "2005",
    DOI =          "10.1093/bioinformatics/bti1114"
}
%
% genutzt
@online{Li2014,
    author =       "Heng Li",
	title =        "Experimenting {FM-index} for multiple human samples",
    year =         "2014",
    URL =          "http://files.figshare.com/1614096/HengLi_20140717.pdf",
	note =         "Accessed: 6th of April 2015"
}
%
% genutzt
% Makinen2010 is reference [11] from Siren2014.
@article{Makinen2010,
    author =       "Veli Mäkinen and Gonzalo Navarro and Jouni Sirén and Niko Välimäki",
    title =        "Storage and Retrieval of Highly Repetitive Sequence Collections",
    journal =      "Journal of Computational Biology",
    volume =       "17",
    number =       "3",
    pages =        "281--308",
    year =         "2010",
    DOI =          "10.1089/cmb.2009.0169"
}
%
% genutzt
% Navarro2007 is reference [6] from Siren2014.
@article{Navarro2007,
    author =       "Gonzalo Navarro and Veli Mäkinen",
    title =        "Compressed full-text indexes",
    journal =      "ACM Computing Surveys",
    volume =       "39",
    number =       "1",
    pages =        "Article 2",
    year =         "2007",
    DOI =          "10.1145/1216370.1216372"
}
%
% genutzt
% Ferragina2009 is reference [10] from Siren2014.
@article{Ferragina2009,
    author =       "Paolo Ferragina and Fabrizo Luccio and Giovanni Manzini and S. Muthukrishnan",
    title =        "Compressing and indexing labeled trees, with applications",
    journal =      "Journal of the ACM",
    volume =       "57",
    number =       "1",
    pages =        "Article 4",
    year =         "2009",
    DOI =          "10.1145/1613676.1613680"
}
%
% genutzt
% Langmead2009 is reference [2] from the grant proposal.
@article{Langmead2009,
    author =       "Ben Langmead and Cole Trapnell and Mihai Pop and Steven L. Salzberg",
    title =        "Ultrafast and memory-efficient alignment of short {DNA} sequences to the human genome",
    journal =      "Genome Biology",
    volume =       "10",
    number =       "3",
    pages =        "R25",
    year =         "2009",
    DOI =          "10.1186/gb-2009-10-3-r25"
}
%
% UNGENUTZT
% McKenna2010 is reference [3] from the grant proposal.
@article{McKenna2010,
    author =       "Aaron McKenna and Matthew Hanna and Eric Banks and Andrey Sivachenko and Kristian Cibulskis and Andrew Kernytsky and Kiran Garimella and David Altshuler and Stacey Gabriel and Mark Daly and Mark A. DePristo",
    title =        "The {Genome Analysis Toolkit}: A MapReduce framework for analyzing next-generation {DNA} sequencing data",
    journal =      "Genome Research",
    volume =       "20",
    number =       "9",
    pages =        "1297--1303",
    year =         "2010",
    DOI =          "10.1101/gr.107524.110"
}
%
% UNGENUTZT
% Garrison2012 is reference [5] from the grant proposal.
@article{Garrison2012,
    author =       "Erik Garrison and Gabor Marth",
    title =        "Haplotype-based variant detection from short-read sequencing",
    year =         "2012",
    note =         "arXiv:1207.3907 [q-bio.GN]"
}
%
% genutzt
% I randomly stumbled over this on twitter... because, oink, why not! =)
% (It may as well apply to our scripts, but not so much to the Delphi MCC because Delphi is not very Linux-y...)
@online{MANIFESTO,
	author =       "Pjotr Prins",
	title =        "Small tools MANIFESTO for Bioinformatics",
	month =        "Feb.",
	year =         "2014",
	URL =          "https://github.com/pjotrp/bioinformatics/blob/master/README.md",
	note =         "Accessed: 27th of July 2015. doi: 10.5281/zenodo.11321"
}
%
% genutzt
@online{RedditSwitchTo38,
	author =       "Gregory M. Cooper",
	title =        "Switch from hg19/build37 to hg20/build38?",
	year =         "2015",
	URL =          "http://www.reddit.com/r/genome/comments/3b3s3t/switch_from_hg19build37_to_hg20build38/",
	note =         "Accessed: 27th of July 2015"
}
%
% genutzt
@article{Taivalsaari2008,
    author =       "Antero Taivalsaari and Tommi Mikkonen and Dan Ingalls and Krzysztof Palacz",
    title =        "Web browser as an application platform: the lively Kernel experience",
    year =         "2008",
    note =         "Technical Report, SMLI TR-2008-175, Sun Microsystems"
}
%
% genutzt
@article{ASCII,
    author =       "Fred W. Smith",
    title =        "Revised {U.S.A.} standard code for information interchange",
	month =        "November",
    year =         "1967",
    journal =      "Western Union Technical Review"
}
%
% genutzt
@article{Siren2009,
    author =       "Jouni Sirén",
    title =        "Compressed Suffix Arrays for Massive Data",
    journal =      "String Processing and Information Retrieval, 16th International Symposium,
               {SPIRE} 2009, Saariselkä, Finland, August 25-27, 2009 Proceedings",
    pages =        "63--74",
    year =         "2009",
    DOI =          "10.1007/978-3-642-03784-9_7"
}
%
% genutzt
% quote this for primary index in BWT (instead of dollar sign)
@article{Nelson1996,
    author =       "Mark Nelson",
    title =        "Data Compression with the {Burrows--Wheeler} Transform",
    journal =      "Dr. Dobb's Journal",
	month =        "September",
    year =         "1996"
}
\end{filecontents*}

% Configurations
\graphicspath{{figs/}}
\AtBeginDocument{\renewcommand{\bibname}{References}}

\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0cm}
\raggedbottom
% \setkomafont{subsection}{\normalfont\sffamily}

% Eins og templatið á að vera
% \setkomafont{captionlabel}{\itshape}
% \setkomafont{caption}{\itshape}

% Mun fallegri lausn
\setkomafont{captionlabel}{\itshape}
\setkomafont{caption}{\itshape}
\setkomafont{section}{\FloatBarrier\Large}
\setcapwidth[l]{\textwidth}
\setcapindent{1em}


% Times new roman
%\usepackage[T1]{fontenc}
%\usepackage{mathptmx}

%%%%%%%%%%% MODIFY THESE LINES ONLY %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\thesisyear{2016}       					% Year thesis submitted
\def\thesismonth{January}						% Month thesis submitted
\def\thesisauthor{Tom Willy Schiller}			% Thesis authoreiningaraðferðinni
\def\thesistitle{Reference Graph Construction and Merging for Human DNA}	% TODO :: Check if title of thesis still has something
%                                                                                     to do with what we actually ended up doing
\def\thesistitlelineone{Reference Graph Construction}	% Front page title of thesis (line one)
\def\thesistitlelinetwo{and Merging for Human DNA}	% Front page title of thesis (line two)
\def\thesisshorttitle{Reference Graphs for Human DNA} 	% Short title of thesis (50 characters including spaces)
\def\thesiscredits{60} 							% Credits awarded for the project
\def\thesissubject{Computer Science}			% Thesis subject
\def\thesiskind{M.Sc.}							% Masters of PhD thesis
\def\thesiskindformal{Magister Scientiarum}		% Masters of PhD thesis
\def\thesisnroftutors{1}						% Number of tutors
\def\thesisschool{School of Engineering and {Natural Sciences}}		% School
\def\thesisfaculty{Industrial Engineering, Mechanical Engineering and Computer Science}	% Faculty
\def\thesisfacultybroken{Industrial Engineering,\\Mechanical Engineering and Computer Science} % I added this -
%                                                                                                Faculty but broken at a nice place
\def\thesisaddress{VR-II, Hjarðarhaga 2-6}		% TODO :: Office address: Tæknigarður, Dunhagi 5 or VR-II, Hjarðarhaga 2-6
%                                                         depending on who to trust... (both from hi.is)
\def\thesispostalcode{107 Reykjavík}			% Office address
\def\thesistelephone{+354 525 4000}					% Office telephone
\def\thesispublisher{XX}						% (Publisher, not in use)
\def\thesistutors{Páll Melsted \\ (imagine some text here)}					% TODO :: Fill in advisors
\def\thesisrepresentative{(imagine some text here)}				% TODO :: Fill in representative
\def\thesiscommittee{XXNN4 \\ XXNN5}			% (Committee name, not in use)
\def\thesiskeywords{genome sequencing, pan-genome indexing, read alignment, Burrows-Wheeler transform, graph indexing}		% TODO :: Read over keywords and make sure they actually correspond to what we are doing here
\def\thesisdedication{“And above all, watch with glittering eyes the whole world around you,\\
because the greatest secrets are always hidden in the most unlikely places.\\
Those who don't believe in magic will never find it.”\\
–- Roald Dahl}
\def\thesisPrinting{Háskólaprent, Fálkagata 2, 107 Reykjavík}

% Function to add footer to frontpage
\newcommand\BackgroundPic{
\put(0,0){
\parbox[b][\paperheight]{\paperwidth}{
\vfill
\centering
\hspace*{-0.42cm}
\includegraphics[width=\paperwidth,height=\paperheight,
keepaspectratio]{evo_hi_footer.png}
}}
\setlength{\unitlength}{\paperwidth}
\hspace*{-0.3cm}
\begin{picture}(0,0)(0.002,-0.15)
\put(0,0){\color{white}\parbox{1\paperwidth}{\centering\bfseries\sffamily \Large Faculty of \thesisfacultybroken \\
University of Iceland\\
\thesisyear}}
\end{picture}
}

\def\pipe{\texttt{|}}

\begin{document}

\begin{titlepage}
\thispagestyle{empty}
% \AddToShipoutPicture*{\BackgroundPic}
%
\begin{center}
\vspace*{1cm}
\hspace*{-1cm}
%\hspace*{0.6cm}
\includegraphics[width=43.6mm]{evo_ui_logo.png}\\
\vspace*{3.0cm}
\hspace*{-1.05cm}
%\hspace*{0.58cm} % TODO this is the hacked-together title on the title page -
%                       so check if the current title is actually still the same
\huge \sffamily \bfseries \thesistitlelineone \\
\hspace*{-1.25cm}
%\hspace*{0.58cm}
\huge \sffamily \bfseries \thesistitlelinetwo

\vspace*{5.5cm}
\hspace*{-1.35cm}
%\hspace*{0.25cm}
\normalfont \Large \sffamily \thesisauthor
\AddToShipoutPicture*{\BackgroundPic}
\vfill

\end{center}

\newpage 
\thispagestyle{empty} \mbox{}
\newpage
\vspace*{1.35cm}
\thispagestyle{empty}
\begin{center}

\Large \textbf{\sffamily{\MakeUppercase{\thesistitlelineone}}} \\
\Large \textbf{\sffamily{\MakeUppercase{\thesistitlelinetwo}}} \\

\vspace*{1.7cm}
\sffamily{\thesisauthor} \\
\vspace*{1.8cm}
\normalsize \thesiscredits~ECTS thesis submitted in partial fulfillment of a \\
\textit{\thesiskindformal} degree in \thesissubject

\vspace*{1.0cm}
\large
Advisors \\ \thesistutors \\ \vspace*{1.04cm}

Faculty Representative \\
\thesisrepresentative

\vspace*{0.4cm}
\phantom{M.Sc. committee} ~\\
% TODO :: if you want the thesis committee, then replace this by \thesiscommittee with \
% (oh, and the ~\\ in the previous line then should also be a \\ without tilde)
\phantom{thesiscommittee }

\vfill
Faculty of \thesisfacultybroken \\
\thesisschool \\
University of Iceland \\
Reykjavik, \thesismonth~\thesisyear
\newpage
\end{center}
 \newpage
 \thispagestyle{empty}
 \mbox{} \vfill
 % \setcounter{page}{0} \renewcommand{\baselinestretch}{1.5}\normalsize
 \sffamily{\thesistitle} \\
 \sffamily{\thesisshorttitle} \\ % TODO :: maybe take the shorttitle out here if I have shorttitle == longtitle
 \thesiscredits ~ECTS thesis submitted in partial fulfillment of a \thesiskind~degree in \thesissubject
\\ \\
Copyright \textcopyright~\thesisyear~ \thesisauthor \\
All rights reserved \\


Faculty of \thesisfaculty \\
\thesisschool \\
University of Iceland \\
\thesisaddress \\
\thesispostalcode \\
Iceland

Telephone: \thesistelephone \\ \\
\vspace*{\lineskip}

Bibliographic information: \\
\thesisauthor, \thesisyear, \thesistitle, \thesiskind~thesis, Faculty of \thesisfaculty, University of Iceland. \\

Printing: \thesisPrinting \\
Reykjavik, Iceland, \thesismonth~\thesisyear \\
\newpage
\thispagestyle{empty} \mbox{}
\vfill
\begin{center}\textit{\thesisdedication}\end{center} \vspace*{5cm}
\vfill 

\thispagestyle{empty}
\cleardoublepage
\end{titlepage}


% \dedication{\textit{Dedication} \small \\ Tileinkun má sleppa og skal þá fjarlægja blaðsíðuna. \\
% Tileinkun skal birtast á oddatölu blaðsíðu (hægri síðu).}
\pagenumbering{roman}

\setcounter{page}{5}
\section*{\huge Abstract}

% TODO :: Útdráttur á ensku sem er að hámarki 250 orð (currently, I have 161 words).
%         NICHT VERGESSEN! Englische und Isländische Abstracts müssen immer den gleichen Inhalt haben!

The focus of this thesis lies on the computational challenges faced when reading out human DNA. 
In particular, reads of the DNA are commonly aligned with the help of a reference string. 
In this thesis, the opportunities and difficulties of using a graph reference are explored instead. \\
Several approaches for using graph references have already been proposed, 
but none of them are used widely in the field. 
In this thesis the existing approaches are therefore compared and 
new algorithms are developed which are intended to help spread 
the usage of graph references further. 
These algorithms are then collected into the Graph Merging Library GML, 
which can be used to visualize their behaviour to further the understanding 
of these methods. 
In addition to being used for the alignment of reads to reference graphs, 
genomic graphs can also arise in other situations. 
The newly implemented algorithms are general enough to also be of use in these scenarios. \\
Overall, this thesis constitutes a step in the direction of adopting 
the more complicated but also more powerful graphs rather than the sequential string data 
within the field of DNA analysis.

\vspace*{1cm}
\section*{\huge Útdráttur}

% TODO :: Hér kemur útdráttur á íslensku sem er að hámarki 250 orð.
%         Reynið að koma útdráttum á eina blaðsíðu en ef tvær blaðsíður 
%         eru nauðsynlegar á seinni blaðsíða útdráttar að hefjast á oddatölusíðu (hægri síðu).
%         NICHT VERGESSEN! Englische und Isländische Abstracts müssen immer den gleichen Inhalt haben!

(imagine some text here)

\newpage

% \chapter*{Preface}
%
% TODO :: check this makes sense for us
%
% At HÍ, the preface can be left out completely.
% In general, a preface describes the student's role in the work as presented.
% A preface could consist of just one or two sentences.
% Each write-up is tightly focussed on expressing who did what.
%
% For more information, see:
% D:\evo-devo\documents\templates\thesis_sample_prefaces.pdf
%
%
% This thesis is original, unpublished, independent work by the author, \thesisauthor.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\addcontentsline{toc}{chapter}{List of Algorithms}
% TODO :: check if the previous line still works (it add the "List of Algorithms" as entry to the main tableofcontents, 
% but it does it AFTER that page, so basically as long as the list of algorithms fills less than one whole page everything is peachy,
% but if we end up with several hundred algorithms and spill over the page, then the page displayed in the main table of contents
% will probably be wrong

\chapter*{Glossary}
\markboth{GLOSSARY}{}
\addcontentsline{toc}{chapter}{Glossary}

% ====================================================================================================================================
%                                                                                                                             GLOSSARY
% ====================================================================================================================================

% TODO :: hier nochmal drüber schauen und sehen, ob wir noch irgendwelche Erklärungen verbessern können,
%         oder noch irgendwelche Begriffe hinzufügen sollten

% TODO :: check if this format as invisible table is actually really okay with everyone...
%         I mean, it looks rad, but maybe it is not official enough?

% TODO :: measure once more if the final table width is exactly the width of text on the page,
%         I don't want this table here to have drastically different margins
%         (to do this just add a lot of continuous text before the table and see if with the
%         current right paragraph width it ends at the exact same margin on the right side)

% TODO :: Check if all the stuff here is arranged alphabetically.
%         I want it to be alphabetically, so I will start it alphabetically,
%         and it would look really weird to have it nearly-but-not-quite-alphabetically.
%         so yeah, just look over it one more time. ;)

% Í þessum kafla mega koma fram listar yfir skammstafanir og/eða breytuheiti. 
% Gefið kaflanum nafn við hæfi, t.d. Skammstafanir eða Breytuheiti. Þessum kafla má sleppa ef hans er ekki þörf. \\
% The section could be titled: Abbreviations, Glossary, Variable Names, etc.

\begin{tabular}{r p{12cm}}

% TODO :: maybe also add the following:
% edits
% FASTA / FASTQ / GFA / STPU; for each of these, send the reader to the Implementation chapter, Data Formats section
% de novo assembly :: assembly without a reference https://en.wikipedia.org/wiki/De_novo_transcriptome_assembly
% lf-mapping :: last-to-front-mapping or last-to-first-mapping (meaning that the relative positions in the BWT column and in the first column conincide)

\textbf{bps: } &
The pairwise occurring essential building blocks of DNA are referred to as \textit{basepairs.}
% TODO :: actually use this abbreviation somewhere in the text... I think the text that used it has been taken out lol ^^ roflderhatlolgesagt
\\ \\

\textbf{bubble: } &
A \textit{bubble} refers to short alternative paths in a genomic graph, such as 
A[C,G]T, which refers to both ACT and AGT. 
We sometimes wish for data formats to enable a simple inclusion of small \textit{bubbles} without 
the explicit generation of nodes and edges for them, 
as that would lead to a lot of overhead and decrease human-readability, 
while the opposite is also often wished for,
to simplify algorithmically working with the format \citep{specGFA1}.
\\ \\

\textbf{BWT: } &
The \textit{Burrows--Wheeler Transform} is a string compression technique \citep{Burrows1994} 
which is used during the read alignment. For a good introduction to the usage of the BWT 
in bioinformatics, see \citet[chapter 7]{bioinfoalgorithms}.
\\ \\

\textbf{CIGAR: } & 
The \textit{CIGAR string} is a sequence of lengths and operations 
associated with these lengths that is used when aligning one string to another. 
Commonly, they contain the operations \texttt{M} for direct match/mismatch, 
\texttt{I} for insertions and \texttt{D} for deletions. 
However, many additional operations for skipping bases, 
clipping, padding and much more have been proposed 
for various projects \citep{specGFA1,SAM2009,specSAM}.
\\ \\
% a really, REALLY good explanation for CIGAR strings (that I can sadly not quote as it is "just some random wiki":
% http://genome.sph.umich.edu/wiki/SAM#What_is_a_CIGAR.3F
% however, the thing that I actually am citing here (the SAM specification) is at least not too bad either,
% as it nicely defines how the CIGAR is built up

\textbf{indel: } & 
\textit{Insertions} and \textit{deletions} are referred to as \textit{indels}. While 
edit operations merely locally change a character within a string, an insertion or 
deletion affects the index of all the following characters, which is why indels 
usually require more effort to be accurately taken into account.
\\ \\

$k$\textbf{-mer:} &
A nucleotide sequence of length $ k $.
\\ \\

\end{tabular}

\begin{tabular}{r p{12cm}}

\textbf{pipeline: } & 
The \textit{analysis pipeline} refers to any set of programs 
that are used sequentially to reconstruct the full genetic information 
of the individual whose DNA was read out based on a vast amount of short reads and a 
known reference, or to create a list of differences between the individual's DNA 
and the reference.
\\ \\

\textbf{rank: } & The \textit{rank} operation counts how often a given character occurs 
in a sequence before a certain index. See page \pageref{def:rank}.
\\ \\

\textbf{read: } & 
An individual's DNA is often not read out as one continuous string, 
but as large amount of short pieces of DNA. 
These short pieces are referred to as individual \textit{reads.}
\\ \\

\textbf{run-length encoding:} &
A simple method for encoding sequential data by replacing any runs of several 
identical elements with an integer counting the amount of identical elements and 
just one of the elements.
\\ \\

\textbf{select: } & The \textit{select} operation returns the index in a sequence 
at which a given character occurred a certain amount of times. See page \pageref{def:select}.
\\ \\

\textbf{self-index:} & 
A \textit{self-index} is a data structure that 
stores sequential data in a compressed form, enables the retrieval of any part of that data without needing 
to access the original data at all, 
and allows efficient pattern searches to be able to quickly perform typical work on the contained data.
\\ \\

\textbf{snip:} &
A \textit{single nucleotide polymorphisms} is a change of a single nucleotide. 
Snips are rather straightforward to work with, as the location of the surrounding data is not affected.
\\ \\

\textbf{variant calling:} & 
The process of finding all nucleotide differences 
between an individual's DNA and a reference. The differences or \textit{variants} are usually reported as positions 
within the reference, and are often given together with confidence estimates.
% TODO :: find a reference for this! (we got it from VariantCallingLecture.pdf in D:, but that is not a good enough source)
\\ \\

\end{tabular}

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

% ====================================================================================================================================
%                                                                                                                      ACKNOWLEDGMENTS
% ====================================================================================================================================

% TODO :: das hier könnte man auch nochmal überarbeiten
%
% Í þessum kafla koma fram þakkir til þeirra sem hafa styrkt rannsóknina með fjárframlögum, aðstöðu eða vinnu. 
% T.d. styrktarsjóðir, fyrirtæki, leiðbeinendur, og aðrir aðilar sem hafa á einhvern hátt aðstoðað við gerð verkefnisins, 
% þ.m.t. vinir og fjölskylda ef við á. Þakkir byrja á oddatölusíðu (hægri síðu).
%
%
% Examples:
%
% Many thanks to all that assisted me throughout the completion of this work, 
% including my supervisor and own professors at the University of Iceland. 
% Without their time, encouragement and support, such a project would not have been possible.
% 
% I would like to thank Ragnar Sigurðsson and Jan Valdman for their guidance and
% support and Fjóla Jónsdottir for insightful comments. I also would like to thank
% Simon Klüpfel for fruitful discussions. Finally I would like to thank my family and
% friends, especially Ragnheiður Einarsdóttir, for their support and patience.
% Thank you.

I would like to thank my tutor, Páll Melsted, for his guidance, support and advice. 
I also would like to thank Mareva Nardelli, Kristinn Ólafsson, Rosemary Milton and Karl Helgason 
for inspiring me to continue working even when facing difficult problems. 
Finally I would like to thank my family for their ongoing support and curiosity.

Thank you all.

\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}

% ====================================================================================================================================
%                                                                                                                         INTRODUCTION
% ====================================================================================================================================

The aim of this thesis is to help improve current approaches from the field of human genome sequencing, 
in which the genome of an individual human being is read out using a combination of 
powerful machines and increasingly complex software algorithms. 
After the sequencing is complete, a list of the variations between an individual's genetic information 
and a reference genome can be produced in a process known as variant calling. 
The list can then be analysed to find previously unknown links between genetic variants and 
diseases throughout large population groups, but it can also be used within clinical tools, allowing the 
diagnosis of uncommon diseases. \\
Many technological advances in recent years have already made it possible to sequence the whole genome 
of an individual for under 1000 USD, 
% TODO :: citation needed
but ideally that price should be reduced even more to simplify further research and to make such diagnosis 
tools available for more people. At the same time, the quality of these methods should also still be improved, 
as the process most commonly used inherently generates many errors.
% TODO :: citation needed

\section{The Role of the Reference}

The process that currently is most often used 
% TODO :: citation needed
is based on a machine sequencing several strands of the 
individual's DNA in many short pieces, called reads. 
A pipeline of several software tools then aligns these reads to a reference genome, 
with the aim of either generating the full individual DNA or a list of variations 
from the reference that has been used. \\
The reference that the software pipeline uses is often a genomic sequence string. 
It represents the average DNA of individuals whose DNA has previously been read out, 
as the DNA of one individual should be similar to the DNA of others.
% TODO :: add picture: reference string generated from several references (that is, the picture contains lots of references,
% that are then mushed into one... leaving out some details.

It is necessary to use such a reference genome, as 
building up the individual DNA just from the reads themselves without a reference is a highly complex problem 
which cannot be solved unambiguously due to many repetitive regions within the DNA. 
Even reconstructing a very short DNA string just from the reads without a reference can lead to drastically wrong 
results, as can be seen in figure~\ref{fig:evo_intro_assembly_wo_reference}. 
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_intro_assembly_wo_reference.png}
\caption[Assembling an individual's DNA from small reads without a reference]{Problems with assembling an individual's DNA from small reads without a reference. Four possible reconstructions of the individual's DNA are shown, which all correspond to the observed reads, but are different from the actual DNA of the individual.} \label{fig:evo_intro_assembly_wo_reference}
\end{figure}
One purpose of the reference is therefore to guide the construction of the individual DNA in a meaningful way.
% TODO :: add picture and some text, saying: E.g. if we have the reads BLA then we could assemble them like BLA BLA,
% but with a reference we get BLAAABBEL...

Another purpose of using a genomic reference becomes clear 
in the case of diagnosing a disease in a patient. 
It is usually not very helpful to build the patient's full individual DNA, as the plain DNA string without 
annotations would not give us any information. 
% TODO :: citation needed
Instead, we are interested only in the variations from the norm, 
as these might be known to be associated with a specific disease. 
By aligning the reads to the reference, the software pipeline can keep track 
of the encountered differences and in the end give out a list of differences between the individual and the reference. 
An example for the process leading to such a list of differences 
can be seen in figure~\ref{fig:evo_intro_general_variation_calling_process}. 
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_intro_general_variation_calling_process.png}
\caption[General variation calling process]{General process for generating a list of variations between the individual DNA and the reference.} \label{fig:evo_intro_general_variation_calling_process}
\end{figure}
The alternative would be to construct the entire individual genome, and upon being done with this enormous task to 
compare the genome with a reference to find all the differences. This comparison however would be a sizeable 
and difficult alignment problem in its own right, so that it is more sensible to align the reads directly to the reference 
and give out the differences that were found in that way.

The reference therefore serves both as a blueprint while aligning the reads, 
as well as an anchor point to which variations within the individual string can be marked.

\section{The Reference Graph}

Traditionally, the reference is a compressed genomic string. 
In its simplest form it can be thought of as a plain text containing only the letters 
A, C, G and T. \\
It is rather straightforward to use such a reference string. 
Each location within the string can clearly be identified by its position---that is, 
its distance from the start. \\
Also, there are very simple file formats available to work with 
such references. One of these is the FASTA format, which includes the genomic 
strings together with lines for free text comments and with line breaks which 
make it easier to display the file in even low level text editors.
% TODO :: citation or link needed for FASTA (also, is it called FATA

The simplicity of reference strings however also has disadvantages. 
Most importantly, when aligning reads to a reference string, we can only align 
the reads to the average human genome, instead of aligning them to all known 
variations of the human genome at once. 
Not taking the known variations into account can lead to worse alignment results 
than would be possible otherwise, as the average genomic string may simply not 
include the particular mutations that have occurred for both the individual 
whose DNA is supposed to be read out as well as for another individual whose 
variations from the reference are known.

This is not just a theoretical consideration. 
Researchers now have access to annotated population variations, which augment the reference sequence string. 
These annotated variations can be found in databases of common variations, such as the publicly available  
dbSNP\footnote{\,\,\,\url{http://www.ncbi.nlm.nih.gov/SNP/}} 
and SNPedia\footnote{\,\,\,\url{http://www.snpedia.com/}}. 
Even the latest release of the Human Reference Genome, 
which is designated GRCh38
\footnote{\,\,\,\url{http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/}}, 
contains alternative haplotypes for some complex regions, 
which are essentially known variations of the rigid reference string. \\
Considering that many alternative references are available and a lot of short variations are known, 
it becomes clear that the human genome should be thought of as a collection of individual genomes 
rather than a single rigid reference genome.

Algorithmically, this corresponds to viewing the reference as a graph rather than a linear genome. 
On this graph, each variation can be represented as a branch which points to the available alternatives. 
Thereby each personal genome is represented by a path through this variation graph, 
as can be seen in figure~\ref{fig:evo_intro_three_ref_seq_align}. \\
% TODO :: in the following picture, also add a graph interpretation underneath what I am seeing
%         (I want to see nodes and edges!)
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_intro_three_ref_seq_align.png}
\caption[Schematic visualization of three paths through a variation graph]{Schematic visualization of three paths through a variation graph, each representing an individual's genome.} \label{fig:evo_intro_three_ref_seq_align}
\end{figure}
This approach simplifies the identification of common variants, as a variant caller can 
ideally just note which path has been taken through the reference graph instead of having to 
explicitly state which differences have been found in which positions. \\
Using a reference graph also makes it possible to pick up combinations of variants 
which would currently be lost in the alignment step. 
Especially the calling of indels, which are usually harder to find and identify than edits, 
can benefit from having local access to several alternative references which contain 
already known variants \citep{Albers2010}.

Another advantage of using a reference graph presents itself when the continuous use of 
references over time is considered. 
When a new version of the human reference genome is published, 
it can take several years for research projects to adapt it, 
as existing pre-processed, aligned and annotated files all have to be updated \citep{RedditSwitchTo38}. \\
If instead a reference graph was underlying all alignments, it would be conceivable 
to update parts of that reference graph without changing the overall indexing of the entire structure, 
making it possible to seamlessly work with different versions of the entire structure. 
A possible way for a graph implementation to support this behaviour is by using a designated main 
reference that is unlikely to change often as the main indexing authority and by adding on top of it 
variations based on other references which can be changed out without changing the overall index.

Unfortunately, to fully implement a reference graph rather than a linear reference 
it is necessary to make drastic changes to all of the existing tools in the analysis pipeline. 
Such changes include the handling of new file formats which are capable of containing graph data, 
as well as changing the internal algorithmic logic of these tools to work on graphs.

\section{Thesis Motivation}

The focus of this thesis is therefore on the creation of an overview of the existing 
approaches that can be used for the implementation of genomic graphs, 
as well as on the exploration of new ways of working with genomic graphs, 
such as merging compressed graphs implicitly or explicitly. \\
The goal hereby is not mainly to create a new read alignment tool that uses 
a reference graph, but rather to create and implement new algorithms that can 
be used in conjunction with such graphs in the future.

Both the newly created algorithms as well as already established ones 
for the conversion of graphs between different internal formats 
will be implemented in such a way that their inner workings 
are directly observable. This simplifies the future usage of these algorithms 
within the scope of various parts of the analysis pipeline.

\chapter{Background}
% was ANDERE Menschen bisher so getan haben
%
% ====================================================================================================================================
%                                                                                                                           BACKGROUND
% ====================================================================================================================================

Nowadays there are many approaches for aligning short reads to a reference string, 
and even some work into the alignment of reads to a reference graph has already been 
undertaken. 
We will first focus on the BWT for strings, which is the basis for the more advanced 
XBW approaches for genomic graphs considered later on.

\section{Burrows--Wheeler Transform for Strings}

The core problem of read alignment to a reference string is simply that a 
lot of data needs to be worked on, as both the reference itself is very big 
and the amount of reads is enormous. \\
This actually leads to two distinct problems, the first being that the 
memory requirements for storing any of the required data are huge, 
and the second being that the time that is spend aligning reads for 
even just a single individual is very long.

To counteract the first problem, the data that is worked on is usually compressed, 
which means mostly that the string reference is being compressed in a certain way. 
Compressing the reads is not as helpful as compressing the reference, 
due to the fact that we can re-use the same compressed reference for aligning the 
reads of another individual later on, without having to execute the lengthy 
process of compressing the reference twice, while the reads will be different 
each time and therefore compressing them would mean that they have to be compressed again the next time around. \\
A method that has been applied very often to enable such a compression is the utilization of the 
Burrows--Wheeler Transform \citep{Burrows1994}, which 
is a reversible reordering of a 
repetitive text with few runs into a text of the same length that contains more runs 
and can therefore be easily compressed by run-length encoding or other encoding schemes. 
A simple example of this behaviour of achieving more easily compressable strings through the application of the BWT 
can be seen in table~\ref{table:evo_background_bwt_run_enc}. 
\begin{table}[htb]
\centering
\caption[Run-length encoding comparison between a repetitive string and its BWT]{Run-length encoding comparison between a highly repetitive string and its BWT. In this example, the run-length encoding of the string itself does not reduce the size, while the run-length encoding of the BWT reduces the size by 29\%.}
   \begin{tabularx}{\textwidth}{ | X | X | }
   \hline
   AGCAGCAGCCTTCTTAGCCTT & \textbf{Original string} \\
   AGCAGCAG\,2C\,2TC\,2\,TAG\,2C\,2T & \textbf{Original string run-length encoded} \\
   \hline
   TCTCGGGGCTCAAAATTTCCC & \textbf{BWT} \\
   TCTC\,4\,GCTC\,4A\,3\,T\,3C & \textbf{BWT run-length encoded} \\
   \hline
   \end{tabularx}
\label{table:evo_background_bwt_run_enc}
\end{table}
As the transformed text contains all the information necessary to recreate the 
original text, it is not necessary to store the original text at all.

A simple algorithm to generate the BWT of a string uses 
the concept of \textit{cyclic rotations}. 
The $ n $th cyclic rotation of a string is a reordering of the string in which its first $ n $ characters 
are taken away from the beginning of the string and are instead inserted at its end. \\
To generate the BWT of a string $ s = c_1 c_2 ... c_k $ of characters $ c_i $, 
we can generate all the cyclic rotations from $ n = 0 $ up to $ n = k - 1 $, and 
put the resulting strings on a list. 
We then sort this list alphabetically, and write out the resulting list of strings 
in such a fashion as to have each of the strings on its own row, exactly underneath each other. 
This creates a matrix of characters, of which we take the last column. 
Read from the top to the bottom, this last column is the BWT of string $ s $. 
This algorithm is also illustrated in figure~\ref{fig:evo_fig_bwt_with_cyclic_rotations}.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_bwt_with_cyclic_rotations.png}
\caption[Generating the BWT of a string]{Generating the BWT of the string $ s = \textup{AGCAGCCTTAGC\$} $. After generating a list of cyclic rotations, the list is sorted, and the last column of the matrix formed by this sorted list reveals the BWT of $ s $ to be \textup{CT\$CGGGCAAATC}.} \label{fig:evo_fig_bwt_with_cyclic_rotations}
\end{figure}

It is customary to append a certain character to the end of the string before generating its BWT, 
with that character being different from all other characters in the string. 
A usual choice for this character is the dollar sign “\$”. \\
The reason for appending such a character is the requirement of being able to reconstruct the original string 
from the BWT, which allows us to discard the original string after encoding it using the BWT 
and compressing it with run-length encoding. 
Without a dedicated character telling us where the end of the string is located, 
the matrix of alphabetically sorted cyclic rotations could be reconstructed from the BWT, 
but it would be impossible to decide which one of the rows in that matrix represented the original 
string, as cyclic rotations based on each row would alphabetically sorted to the exact same matrix. 
Instead of adding a distinctly different character to the end of the string, 
it is therefore also possible, although not customary, to simply store an integer with the BWT containing 
the number of the row within the matrix which contains the original string \citep{Nelson1996}.

As mentioned before, it is crucial that the BWT is a reversible reordering of a string, 
such that the original string can be obtained from the BWT. 
A simple algorithm for recreating the original string given its BWT starts by reconstructing the alphabetically sorted 
matrix of cyclic rotations as empty $ n {\times} n $ matrix, where $ n $ is the length of the BWT. 
The last column of the matrix is then filled with the BWT, spelled out vertically. 
All the rows in the matrix are then sorted alphabetically. 
Now, the second-to-last column of the matrix is filled with the BWT, 
and again all the rows in the matrix are sorted alphabetically. \\
This inserting of the BWT in the right-most empty column and subsequent alphabetical sorting of all rows 
is continued until the matrix has been filled entirely. 
The original string now consists of the first $ n - 1 $ characters in the row in the matrix which 
ends with the special character, 
usually the dollar sign.

% TODO :: add figure to the BWT reverting?

As noted before, the given algorithm for the creation of the BWT is rather simple, 
and more efficient algorithms exist which do not require building up the entire matrix of cyclic rotations. 
Likewise, the algorithm for regaining the original string from the BWT is not customarily 
used in its form here in practice. \\
Actually, many current alignment tools such as BWA-MEM \citep{Li2013} and Bowtie \citep{Langmead2009} which use 
the BWT to compress the reference actually 
work on the reference in this compressed form directly, without having 
to recreate the original text in its entirety in memory.

As memory has gotten cheaper over time and machines have become more 
well-equipped, memory consumption nowadays is not as much of a concern 
as it once was, when just fitting the reference into memory 
took a lot of ingenuity. \\
% TODO :: citation needed
However, the memory utilization of alignment tools is still important, 
as further reductions in the overall amount of memory necessary would 
mean that the alignment could be done using smaller and therefore cheaper 
machines, which would open up more real-world applications for sequence analysis 
for which its price currently is still too high.
Also, a focus on how the available memory is used could lead to new alignment 
tools that make more efficient use of the special memory that is more readily available 
for the processor, such as using the L1, L2 and L3 cache rather than sending a lot of 
requests to the slower general RAM. \\
Therefore, research into methods reducing the memory requirements of alignment tools 
is still ongoing.
% TODO :: citation needed? maybe?

\section{The Role of Graphs in Read Alignment}

Initial use of graphs in read alignment was restricted to representing 
the outcome of the alignment phase, in particular the fully assembled DNA \citep{Myers2005}.

Short reads usually do not make it possible to fully infer the actual structure of the genome that is read out, 
as different possible structures could have lead to the same reads, especially when considering that 
there are many errors within the reads that need to be accounted for. \\
Therefore, often the reads are aligned to a reference and the best fitting position is simply 
assumed to be the real origin of the read. However, as the decision to choose one position over 
another can be quite arbitrary, graphs can be used as read alignment output which indicate how the reads 
are linked together. The true individual DNA that was read out is then one possible path through the graph, 
while other possible paths through the graph exist that are merely artefacts of the sequencing process. \\
As such a graph can be quite difficult to work with, not many alignment tools are producing these 
structures. Instead, the default is usually to just choose the best fitting position and 
create a read out DNA string. This string will most likely also contain some sequencing artefacts, 
but will be vastly easier to work with in later steps.

Graphs are currently also starting to be used as a way to reduce the size of several read out human 
genomes \citep{Li2014}. \\
The idea stems from the fact that companies or institutions that read out the DNA of several individuals 
and want to store them can run in problems with the immense file size when storing or transmitting the reads and their 
aligned locations. Instead, the original read data could be discarded and only the alignment result could be saved and shared---that 
however would mean that the original data cannot later on be re-interpreted by better algorithms, and can in general 
not be used any more. \\
Using a graph in this case can allow several agreeing reads to be combined into long sequences of unambiguous data, 
while the locations at which uncertainties exist could still be encoded with all the possible alternatives as 
different paths through the graph. Therefore, all complicated read data behaviour is contained in the resulting 
graph, but the size is still vastly reduced when compared to using all reference strings of the population.

\section{Formal Graph Definition}
\label{sec:graph_definition}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                  BACKGROUND: Formal Graph Definition
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We define a \textit{graph} $ G = (V, E) $ as a collection of a set 
of \textit{nodes} $ V = \{ v_1, ..., v_{|V|} \} $ and a set of edges $ E {\: \subseteq \:} V^2 $. 
Each one of these edges is a set $ (u, v) {\: \in \:} E $ with $ u {\: \in \:} V $ and $ v {\: \in \:} V $, 
which we call an edge from node $ u $ to node $ v $. 
As the graphs we are working with are considered to be \textit{directed}, 
we assume edge $ (u, v) {\: \in \:} E $ to 
be distinct from edge $ (v, u) {\: \in \:} E $. 
An example of such a graph can be seen in figure~\ref{fig:evo_fig_graph_example}. \\
% GACGT|,2,T,4;,3,,5 vs. ACCTG|,1,,5;,1,,3 without start and end nodes
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_graph_example.pdf}
\caption[Example graph]{Example graph with 11 nodes and 14 edges.} \label{fig:evo_fig_graph_example}
\end{figure}
For every node $ v {\: \in \:} V $ we define the amount of incoming edges of 
node $ v $, or the \textit{indegree} $ \textrm{in} ( v ) $, as the number of 
distinct edges $ (u, v) {\: \in \:} E $ for any $ u {\: \in \:} V $. 
Similarly, for every node $ v {\: \in \:} V $ we also define the amount of outgoing edges of 
node $ v $, or the \textit{outdegree} $ \textrm{out} ( v ) $, as the number of 
distinct edges $ (v, u) {\: \in \:} E $ for any $ u {\: \in \:} V $. \\
The graphs considered here are \textit{labelled}, which means that a label $ l( v ) $ is attached 
to every node $ v {\: \in \:} V $. 
Every label $ l( v ) $ represents a character 
from the alphabet $ \Sigma $, which is a set of characters. 
In principle, a label could also consist of several characters, 
but we limit each label to exactly one character as we wish to 
represent several characters as nodes with separate labels sequentially connected by edges. 
We refer to such a structure of sequentially connected 
nodes $ u_1, u_2, ..., u_n $ as \textit{path} $ P = u_1 u_2 ... u_n $.

\section{Burrows--Wheeler Transform for Graphs}

Graphs have not only been used to indicate the results of the read alignment. 
The alignment of short reads to a reference graph rather than to a linear reference has 
also been proposed several times.

A necessary prerequisite for aligning reads to a reference graph is 
being able to actually create such a graph in the first place. \\
One of these ways is to take several similar genomic strings and create a graph based 
on them, such that each of the input strings is a path through the resulting graph. 
\citet{Lee2002} proposed a method to create such a graph from input strings, 
independent of the order in which these strings 
are added to the emerging graph.

Storing several very similar references at the same time in a way that not only minimizes 
the storage space, but also enables efficient pattern searches directly on the 
stored data was proposed by \citet{Makinen2010}. \\
The aim of these techniques is to 
minimize both the overhead in storage space and the needed time to perform typical work 
on the references, such as aligning reads to them.

This group of authors continued working with scenarios in which 
several references are merged into a graph, with reads being directly 
aligned to the graph rather than to each reference on its own \citep{Siren2014}. \\
The alignment of reads against the data structure incorporating several references 
is done in a way that is inspired by the Burrows--Wheeler Transform. \\

In particular, to be able to use the Burrows--Wheeler Transform for read alignment, 
a data structure called the suffix array \citep{Puglisi2007} is commonly used, which contains 
information about the locations within the reference. 
Using the suffix array makes it easier to work on the BWT in its compressed form, 
rather than having to reconstruct the original string from it to perform work. 
That is, the BWT together with its suffix array forms a self-index, 
which is a data structure that contains compressed data and makes it possible 
to work on that data directly in its compressed form \citep{Navarro2007}. \\
As indexing a graph based on several references is not as straightforward as 
indexing a single string, which provides an inherent indexing mechanism by 
simply stating the position within the string, the regular suffix array cannot 
be used in the case of encoding a graph rather than a string. \\
However, the plain suffix array is often not used in a practical sense 
without any changes anyway. Instead, it is often compressed in some way or another. 
The reason for that is the immense size of the regular suffix array. 
A common compression technique is to leave out many of the suffix array's parts 
and instead compute them dynamically 
when they are being used rather than storing the entire array in memory. 

When using a reference graph for read alignment with the BWT, it is therefore common 
already to use a structure that simply behaves similar to the suffix array. 
Such a suffix-array-like data structure can also be constructed for graph references.

In particular, efficient support of the following operations is necessary for such a data structure \citep{Siren2014}: \\
Given a pattern, the range within the data structure 
that corresponds to all suffixes of the reference graph that are prefixed with that pattern needs to be found. 
This essentially provides a functionality for finding arbitrary texts, and is used when aligning reads to 
the reference, as that is basically a search for the read itself or similar patterns within the reference. \\
Given a location within the data structure, the corresponding location within the 
reference needs to be located. \\
Finally, given a location within the reference, the actual text at that position 
needs to be extracted.

The main idea is therefore to create a structure capable of fulfilling these requirements, 
as the remainder of the alignment step is then very similar to the alignment against a reference string.

\section{Alignment Without an Actual Graph}

To avoid the complications arising when working with 
a complete graph made from several references, other methods have been proposed 
to achieve similar results without explicitly constructing a reference graph.

One of these is the alignment tool BWBBLE which 
was created by \citet{Huang2013}. \\
It works on the core assumption that most differences in between two references 
are snips, which can be encoded with a specific extended alphabet. 
Then, a modification to the Burrows--Wheeler Transform is made to 
be able to work with a single string reference containing this extended alphabet. \\
BWBBLE also supports insertions and deletions which means that it does support 
more graph-like behaviour, but the core functionality of it is aimed at the extended string reference.

In addition, there have been efforts to more efficiently create the BWT 
of several strings rather than just concatenating them into a single one. 
These can help in understanding the challenges of building the BWT of several references \citep{Holt2014}.

\section{Using Hashes to Encode a Graph}

The hashing approach focuses also on a certain way of pre-processing the references 
to create a certain structure that the reads can then be aligned against. \\
To use this approach, a specific length $ k $ is given, up to which we keep track of 
sequences in the reference graph.

In the pre-processing step, among all the possible references one main reference is chosen. 
Then a hash table based on the references is created which 
assigns each $ k $-mer the positions at which it can be found. 
Such a position is not as straightforward as a 
plain integer that gives the index within a string, as the data can lie on branches 
off the main reference string. \\
This is solved by using an integer together with minor extra information. 
The integer points towards a data block, which can be a part of the main 
reference or a branch of any of the other references that differs from the 
main sequence. The extra information then points towards the location within the block.

In particular, aligning reads to several references at once 
in this way has been proposed by \citet{Schneeberger2009}. \\
In this case, the references are taken together to form a graph 
rather than being used as separate reference strings. 
For this to be achieved, all references are pre-processed in a special way, with one being taken as the 
main reference and the other references being stored as changes to this main reference. 
The pre-processing step results in a data structure that uses $ k $-mers as hashes pointing 
to the specific location at which these $ k $-mers can be found. Reads can therefore be aligned 
by finding parts of them in the hash table, looking up the locations that are associated with 
them, and checking if the entire read can be aligned to that location. \\
The location here is not just an integer pointing to a character within a string, 
as it would be for a single reference, but actually 
a pointer to the particular reference and the location within it. 
To make this indexing possible, the references are split into blocks that 
can quickly be addressed with an integer and locations within those blocks.

When the reads are aligned against the data structure, the output of the alignment step 
can be generated in two ways---either with each read being aligned to the particular reference 
that it agrees with most, or with each read being aligned in the same way but with the 
alignment afterwards being rewritten as if the read was aligned to the main reference. \\
This second option makes it possible to use this approach even as part of an already existing pipeline 
built for a reference string, not a reference graph, if losing some of the benefits of 
the graph alignment is acceptable. However, to fully utilize all the graph information, 
the rest of the pipeline has to be adapted as well.

\section{Extending the Burrows---Wheeler Transform}

Efforts involving changes to and generalizations of the Burrows--Wheeler Transform 
are also aided by the development of the XBW, which is a 
method of compressing trees similar to how the BWT is used to compress sequential data \citep{Ferragina2009}. \\
Each node in the tree has a label that is one character long. 
These labels are stored in the XBW together with a special bit vector, 
which keeps track of whether a given node is a leaf node or not. 
That is, if a node has a successor, then this bit vector takes the value 0, as that node is not the last node 
of its branch. If a node does not have a successor, then the bit vector takes the value 1 for that node. \\

\citet{Siren2014} extended the XBW even further, by introducing a second bit vector that enables nodes 
to have multiple predecessors as well as multiple successors. Therefore, this extended XBW can not only 
store trees, but instead can keep track of general finite graphs. 
In this paper, the extended XBW is stored in two different kinds of data structures, 
which have not been given explicit names there. We refer to these structures as 
node tables and flat tables.

\subsection{Node Tables}
\label{sec:node_table_definition}

We decided to refer to the first of the two data structure as XBW node table, or just node table, 
as it is a 
table containing the extended XBW which contains exactly one column for each 
node of the graph represented by the extended XBW. \\
More formally, we define:

\textbf{Definition} An \textit{XBW node table} is a table with rows for the BWT, 
the prefixes and the values of the bit vectors $ M $ and $ F $, 
representing a finite graph with each column of the table corresponding 
to exactly one node of the graph.

The columns in a node table are sorted alphabetically by the given prefix. 
For any column $ i $ representing node $ v_i $ in the graph, 
the value of the BWT in column $ i $ represents the labels of the nodes 
preceding $ v_i $. 
If several nodes with different labels are preceding the node $ v_i $, 
then the BWT value takes on the concatenation of all these values. 
To make it clearer that a BWT containing several characters actually 
stands for any of these characters, instead of standing for the string 
formed by the characters, we decided to display BWT values containing 
more than one character separated by pipe characters, as can be 
seen in figure~\ref{fig:evo_fig_node_table_example} which is represented 
by node table~\ref{table:evo_node_table_example}. \\
% GCTGGCGAG|,2,TT,5;,2,TGG,7
{
% reduce whitespace in this table (default might be 6pt)
\renewcommand{\tabcolsep}{2pt}
\begin{table}[htb]
\centering
\caption[Node table corresponding to a graph]{Node table corresponding to the graph in figure~\ref{fig:evo_fig_node_table_example}.}
\begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | }
\hline
G & G & G & G & A & C|G & G|T & \textbf{\#} & G & T & T & T & C & C & C & \$ & \textbf{BWT} \\ \hline 
\$ & A & CG & CT & G\$ & GA & GCG & GCT & GGA & GGC & GGG & TGC & TGGC & TGGG & TT & \textbf{\#} & \textbf{Prefix} \\ \hline 
1 & 1 & 1 & 100 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & $\boldsymbol{M}$ \\ \hline 
1 & 1 & 1 & 1 & 1 & 10 & 10 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & $\boldsymbol{F}$ \\ \hline 
\end{tabular}
\label{table:evo_node_table_example}
\end{table}
}
% GCTGGCGAG|,2,TT,5;,2,TGG,7, just like above
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_node_table_example.pdf}
\caption[Graph corresponding to a node table]{Graph corresponding to node table~\ref{table:evo_node_table_example}. The prefixes of the nodes have been provided in small print above the nodes, to make it easier to identify which node in the graph corresponds to which column in the table.} \label{fig:evo_fig_node_table_example}
\end{figure}
% TODO :: make sure that these two (the table and the figure) actually end up on the same page, and are not split over several pages
The value of the prefix in column $ i $ is precisely the prefix of node $ v_i $ in the graph, 
which is a string starting with the label of $ v_i $ and then containing 
the labels of the following nodes in sequence of encountering them traversing 
the graph outwards from $ v_i $.
The value of the cell $ M $ in column $ i $ is a sequence of bits whose length corresponds to 
the amount of successor nodes of $ v_i $, or equally the amount of outgoing edges of node $ v_i $. 
This sequence always starts with the bit “1” and is then followed 
by as many “0” bits as are necessary to achieve the desired length. 
Similarly, the value of $ F $ in column $ i $ is a bit sequence whose length corresponds to 
the amount of predecessor nodes of node $ v_i $. 
When implementing $ M $ and $ F $, they can instead be thought of as arrays of strings, 
which each string containing the characters “1” and “0”, for ease of implementation.

\subsection{Flat Tables}
\label{sec:flat_table_definition}

In contrast to the previously introduced node table, 
we refer to the next data structure as flat XBW table, or just flat table. 
This name is inspired by the fact that the data of each row here is not 
stored as array with elements of various lengths, 
but instead as flat sequence of characters which can be encoded as string 
or as array with elements of fixed size. \\
The definition shows that a flat table consists of similar parts as a node table:

\textbf{Definition} A \textit{flat XBW table} is a table with rows for the BWT, 
the node labels and the values of $ M $ and $ F $, 
representing a finite graph. 
The table is arranged such that each cell contains exactly one character 
for the BWT and the node labels, and such that each cell contains exactly 
one bit for the bit vectors $ M $ and $ F $.

We here often refer to the row containing the node labels as first column row, or short “FC”. 
The reasoning behind this name is that the row of node labels of the graph relate to the BWT row, 
as the first column in the matrix of sorted cyclic rotations does to the last row, which is the traditional BWT. 
That is, the node labels are also the alphabetically sorted BWT, and in fact the columns are sorted in such a 
way that when the prefixes are generated for each column in turn, they are sorted alphabetically as well. \\
As the FC row simply corresponds to the BWT sorted alphabetically, it can easily be calculated 
when the BWT row is known. The BWT however, while being potentially very 
long, only consists of characters from a limited alphabet $ \Sigma $. 
Therefore, storing the FC both in a file and in memory does not take up a lot of storage space. 
This means that we usually accept the small space taken up by keeping it stored in memory, 
instead of recalculating it based on the BWT again and again. 
We can achieve this storage of FC using the array $ C $, which is another structure that can be generated 
from the BWT. \\
In fact, we usually calculate several structures from the BWT, $ M $ and $ F $ which 
can be used in conjunction with the flat XBW table. 
The additionally calculated information includes 
the alphabet $ \Sigma $, a set of all distinct characters encountered in the BWT. \\
We also keep track of the array “ord”, which has the characters of the alphabet as keys as their 
positions in the alphabetically sorted $ \Sigma $ as indices. \\
We finally generate the array $ C $, which also has the characters of the alphabet as keys, 
and whose values are the indices of the first occurrences of these characters within FC. 
The array $ C $ completely encodes FC, so that we do not need to explicitly store FC in any other way. 
Of course, we can do so anyway for a particularly simple implementation.

The bit vectors $ M $ and $ F $ are exactly the same as the corresponding bit vectors in the node table, 
and are each strongly related to one of the other rows. 
In particular, each cell in the $ M $ vector corresponds to a cell, or character, in the FC row, 
as each outgoing edge $ (u, v_i) $ of a node $ u $ corresponds to another cell in the $ M $ vector, and each 
of these outgoing edges shares the same node $ u $ in which it originates, therefore sharing the same label $ l ( u ) $, 
which is precisely the content of the FC row. \\
On the other hand, each cell in the $ F $ vector corresponds to a cell, or character, in the BWT. 
The reason for this is that the graph we are working on is necessarily reverse deterministic 
for this encoding to function properly, which is explored in further depth in section~\ref{sec:why_rev_det}. 
Due to that, the label $ l (u_i) $ for each incoming edge $ (u_i, v) $ into a node $ v $ is distinct, 
as the graph would otherwise not be reverse deterministic. 
Therefore, for every node $ v $ there are as many entries in the BWT, which contains the labels of the preceding nodes, 
as there are entries in the $ F $ bit vector, which keeps track of the amount of incoming edges.

Encoding the same graphs both as flat XBW tables and as node tables 
shows that flat XBW tables are much smaller and therefore easier to store. 
The biggest differences between the two kinds of tables, 
which can both be used to encode genomic graphs, 
are that flat tables do not contain the potentially enormously big prefix cells 
of node tables, and that flat tables do not require any overhead to 
keep track of column boundaries within a row. 
While node tables can contain strings of various sizes in all their cells, 
flat tables contain exactly one character in each cell, such that 
an entire table row can be stored as string without losing information 
about the cell boundaries. \\
This smaller size comes at the cost of simplicity. 
Within a node table, each column clearly corresponds to a node within the graph, 
and a simple manual inspection of the table can lead to various insights into the graph. 
In a flat table however, not every column describes and entire node within the graph, 
as some columns only describe parts of nodes, as e.g. a node with several predecessors 
can have information about its BWT spread across several columns in the flat table. 
We therefore should not even think about whole columns in the flat table, 
but instead of clusters of cells which correspond to nodes. 
We use the $ M $ and $ F $ rows to 
find these cell clusters across different rows, which for graphs with high in- and outdegrees 
are not necessarily located in columns that are even close together.
How cells in a flat table relate to actual nodes in a graph is illustrated via specifically highlighted 
cells in table~\ref{table:flat_table_with_ct}, which corresponds to the graph with highlighted nodes 
in figure~\ref{fig:evo_fig_flat_table_with_ct}. \\
{
% reduce whitespace in this table (default might be 6pt)
% \renewcommand{\tabcolsep}{5pt}
% GCTGGCGAG|,2,TT,5;,2,TGG,7, searching for CTTG
\begin{table}[htb]
\centering
\caption[Flat table corresponding to a graph]{A flat XBW table with cells highlighted that correspond to four nodes in the graph in figure~\ref{fig:evo_fig_flat_table_with_ct} represented by this table. The alphabet for this table, including the labels of the start and end node, is $ \Sigma = \{ \textup{\$, A, C, G, T, \textbf{\#}} \} $. The \textup{ord} array is $ [ \$ {\, \Rightarrow \,} 0, \textup{A} {\, \Rightarrow \,} 1, \textup{C} {\, \Rightarrow \,} 2, \textup{G} {\, \Rightarrow \,} 3, \textup{T} {\, \Rightarrow \,} 4, \textup{\#} {\, \Rightarrow \,} 5 ] $ and the $ C $ array is $ [ \$ {\, \Rightarrow \,} 0, \textup{A} {\, \Rightarrow \,} 1, \textup{C} {\, \Rightarrow \,} 2, \textup{G} {\, \Rightarrow \,} 6, \textup{T} {\, \Rightarrow \,} 13, \textup{\#} {\, \Rightarrow \,} 17 ] $.}
\begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | }
\hline
G & G & G &\cellcolor{purple_bg}\color{purple_fx}G & A & C & G &\cellcolor{green_bg}\color{green_fx}G &\cellcolor{green_bg}\color{green_fx}T & \textbf{\#} & G & T & T &\cellcolor{red_bg}\color{red_fx}T & C & C &\cellcolor{blue_bg}\color{blue_fx}C & \$ & \textbf{BWT} \\ \hline 
\$ & A & C &\cellcolor{purple_bg}\color{purple_fx}C &\cellcolor{purple_bg}\color{purple_fx}C &\cellcolor{purple_bg}\color{purple_fx}C & G & G &\cellcolor{green_bg}\color{green_fx}G & G & G & G & G &\cellcolor{red_bg}\color{red_fx}T & T & T &\cellcolor{blue_bg}\color{blue_fx}T & \textbf{\#} & \textbf{FC} \\ \hline 
1 & 1 & 1 &\cellcolor{purple_bg}\color{purple_fx}1 &\cellcolor{purple_bg}\color{purple_fx}0 &\cellcolor{purple_bg}\color{purple_fx}0 & 1 & 1 &\cellcolor{green_bg}\color{green_fx}1 & 1 & 1 & 1 & 1 &\cellcolor{red_bg}\color{red_fx}1 & 1 & 1 &\cellcolor{blue_bg}\color{blue_fx}1 & 1 & $\boldsymbol{M}$ \\ \hline 
1 & 1 & 1 &\cellcolor{purple_bg}\color{purple_fx}1 & 1 & 1 & 0 &\cellcolor{green_bg}\color{green_fx}1 &\cellcolor{green_bg}\color{green_fx}0 & 1 & 1 & 1 & 1 &\cellcolor{red_bg}\color{red_fx}1 & 1 & 1 &\cellcolor{blue_bg}\color{blue_fx}1 & 1 & $\boldsymbol{F}$ \\ \hline 
\end{tabular}
\label{table:flat_table_with_ct}
\end{table}
}
% GCTGGCGAG|,2,TT,5;,2,TGG,7, searching for CTTG, with colors inverted, just like above
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_flat_table_with_ct.pdf}
\caption[Graph corresponding to a flat table]{Graph corresponding to flat table~\ref{table:flat_table_with_ct}. The nodes highlighted in this graph correspond to the cells highlighted in the flat table.} \label{fig:evo_fig_flat_table_with_ct}
\end{figure}
% TODO :: make sure that these two (the table and the figure) actually end up on the same page, and are not split over several pages
More formally, we can observe that 
if we have a cell in column $ i $ of the FC row, and want to find the other cells corresponding to 
the node which this cell belongs to, then we first of all check the value of $ M $ in column $ i $. 
If it is zero, we go to the left until we reach a column $ j $ which contains a one in $ M $. 
If it is not zero, then we set $ j = i $. 
We now go to the right from cell $ j + 1 $ onwards, until we encounter a one in $ M $ in column $ k $. 
This shows us that all the FC values and all the $ M $ value 
from columns $ j $ until $ k - 1 $, including both $ j $ and $ k - 1 $, 
belong to the same node in the graph. \\
To find the corresponding BWT and $ F $ values, we iterate over all columns of the table from the beginning on 
the left until we reach $ j $, and count the occurrences of ones in $ M $. 
We refer to this operation as computing the \textit{rank} of character 1 in sequence $ M $ until the limit $ j $, 
which we denote as $ \textrm{rank}_1 ( M, j ) $ \citep{Siren2009}.\label{def:rank} 
We then iterate over the columns of the table again from the beginning on the left, 
increasing a counter every time that the bit vector $ F $ contains a one, until we reach 
the result that the rank operation gave us. 
We refer to this as a \textit{select} operation, selecting character 1 in sequence $ F $ until the limit $ l $, 
where $ l $ in this case is set as the result of the rank operation before. In general, 
we denote such a select operation as $ \textrm{select}_1 ( F, l ) $ \citep{Siren2009}.\label{def:select} \\
The result of the select operation is now the index $ h $ of the column in which the BWT and $ F $ values 
of the node we are interested in start. 
We again go on to the right through $ F $ until we encounter the next one in column $ g $, 
and finally know that the node which we were interested in corresponds to the 
FC and $ M $ values in columns $ j $ until $ k - 1 $ and to the BWT and $ F $ values in columns $ h $ until $ g - 1 $. \\
Similar operations can be performed to find the range in FC and $ M $ given an index into the BWT, 
namely ranking the index in $ F $ and calling a select operation on $ M $ to find the corresponding start.

We conclude the explanation of node tables and flat tables by remarking that while each column 
in an XBW node table corresponds to a node in the graph, 
each column in the flat table can be thought of as corresponding to an edge in the graph, 
if the BWT and $ F $ rows are thought of separately from the FC and $ M $ rows. 
That is, each of the columns in the BWT and $ F $ rows corresponds to an incoming edge for some node, 
and each of the columns in the FC and $ M $ rows corresponds to an outgoing edge for some node. 
Therefore, we are always assured that the total length of the BWT, $ F $, FC and $ M $ rows are the same 
within the flat table, as the total amount of outgoing edges from all nodes in the graph is 
the same as the total amount of incoming edges into all nodes in the graph.

\chapter{Methods}
% Lösungsansatz / VON UNS vorgeschlagene Methoden
%
% ====================================================================================================================================
%                                                                                                                              METHODS
% ====================================================================================================================================

To be able to understand the different existing approaches for working with 
reference graphs and genomic graphs in general, we have re-implemented several 
of these approaches myself. These implementations have been done in the programming 
language Python and resulted in several scripts that can be steered 
by a graphical main program written in Delphi. \\
We have then focused on creating the Graph Merging Library GML, which is a 
collection of algorithms written in JavaScript that make it possible to 
work with reference graphs in various ways. JavaScript is not a particularly fast 
language in itself, due to many inbuilt functionalities which are helpful for programming 
in it but slow it down compared to other languages such as C/C++ \citep{Taivalsaari2008}. 
The aim of this library is therefore not to directly provide a means to actually do 
real-world calculations using the entire human genome as reference graph, 
but instead it is focused on showing how different algorithms work, 
and in general to provide a test bed for working with genomic graphs. \\
Nevertheless, porting these algorithms to a faster programming language 
and using them in a production environment should of course be possible.

\section{Data Formats}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                                METHODS: Data Formats
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the course of working on this thesis we used and compared existing data 
formats for genomic graphs as well as designed several new data formats 
to be able to better understand the strengths and weaknesses of 
different possible approaches for encoding graphs.

\subsection{FASTG Format}

There are many formats readily available for genomic data strings 
that can be used when only sequential data is considered. 
One of the most common formats for genomic strings is the FASTA format. 
% TODO :: citation necessary
Its simple structure means that it is human-readable 
and can easily be used by various programs.

However, the aim here is to find data formats that are capable of 
encoding genomic graphs rather than strings. 
Such a format is FASTG, which is 
a FASTA-like format designed to handle graph data \citep{specFASTG}. 

% TODO :: write about FASTG - do not write down the entire standard, 
% but rather one or two important things for someone to understand how we implemented it.

\subsection{GFA Format}

The GFA or Graphical Fragment Assembly format 
has been proposed in 2014 \citep{specGFA1,specGFA2}. 
This young format has a lot of advantages over FASTG, mainly 
that it is a lot more flexible and can easier be read by programs, 
as not much emphasis is put on making it human-readable. 
Therefore, the focus can lie clearly on the needs of developers of advanced software packages, 
and on allowing them to directly store the kind of data that they encounter, 
even if it complicates the format slightly.
% TODO :: citation needed?

% TODO :: write more about GFA - as with FASTG, just a quick overview over the main commands and how they can be implemented

\subsection{Bubble Format}

Having explored the FASTG and GFA format, we decided to also look 
at the bubble notation and base a data format on it. 
It is not supposed to be used for production environments and indeed 
is not complex enough to describe any but the most trivial kinds of graphs. 
However, we still decided to also have a look into this particular format, 
as implementing it in a software package might teach us valuable lessons 
about problems to avoid when designing actual graph formats to be used for real world data.

In the bubble format a genomic sequence is represented as a single continuous string. 
There are no comments within the format, 
and neither newline characters nor in fact any kinds of whitespaces. \\
Any sort of graph representation in STPU is based on bubbles, which can be extended as long as necessary, 
and can be nested. A bubble is started with a “(” character and ended 
with a “)” character. Alternatives within the bubbles are marked with “{\!\pipe}” 
characters. 
A representation of a short and simple graph in bubble notation can be seen in figure~\ref{fig:evo_fig_STPUbubble_f}. \\
Insertions and deletions are represented as bubbles whose alternative route is empty. 
One such deletion can be seen in figure~\ref{fig:evo_fig_STPUinsertion_f}. \\
As remarked before, this format cannot be used to represent arbitrary graphs. 
An example for which it fails to provide a correct encoding can be seen in figure~\ref{fig:evo_fig_STPUcycle_f}. 
Nevertheless, the bubble format is sufficient for simple algorithmic tests.

% \begin{figure}[!htb]
% \centering
% \includegraphics[width=0.47\textwidth]{evo_fig_STPUbubble_f.png}
% \caption[Simple bubble in STPU format]{Simple bubble, represented in STPU as \textup{ACGT(AG\pipe CTT)ATTTC}.} \label{fig:STPUbubble}
% \end{figure}
% ACGTCTTATTTC|,4,AG,8    nicer: AG against C - CGTAGATTC|,4,C,7 (without $ and #)
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_STPUbubble_f.pdf}
\caption[Simple bubble in bubble format]{Simple bubble, represented in bubble format as \textup{CGTA(GA\pipe C)TTC}.} \label{fig:evo_fig_STPUbubble_f}
\end{figure}
% \begin{figure}[!htb]
% \centering
% \includegraphics[width=0.47\textwidth]{evo_fig_STPUinsertion_f.png}
% \caption[Insertion in STPU format]{Insertion, represented in STPU as \textup{ACGT(AG\pipe )ATTTC}.} \label{fig:STPUinsertion}
% \end{figure}
% ACGTATTTC|,4,AG,5    nicer: deletion - CGTAGATTC|,4,,7 (without $ and #)
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_STPUinsertion_f.pdf}
\caption[Deletion in bubble format]{Deletion, represented in bubble format as \textup{CGTA(GA\pipe )TTC}.} \label{fig:evo_fig_STPUinsertion_f}
\end{figure}
% \begin{figure}[!htb]
% \centering
% \includegraphics[width=0.47\textwidth]{evo_fig_STPUcycle_f.png}
% \caption[Cycle in STPU format]{Cycle which cannot be fully represented in STPU, but can be approximated to any wanted length as \textup{ACGTAG(\pipe AG(\pipe AG(\pipe AG(\pipe ...))))ATTTC}.} \label{fig:STPUcycle}
% \end{figure}
% CGTAGATTC|,7,,4 (without $ and #)
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_STPUcycle_f.pdf}
\caption[Cycle in bubble format]{Cycle which cannot be fully represented in bubble format, but can be approximated to any wanted length as \textup{CGTAGAT(\pipe AGAT(\pipe AGAT(\pipe AGAT(\pipe ...))))TC}.} \label{fig:evo_fig_STPUcycle_f}
\end{figure}

\subsection{GML Format}

After implementing the Bubble Format, which represents a very straightforward but rather 
limited approach for encoding graphs, 
we decided to design a different new format 
which could be used to encode genomic graphs. 
As both the FASTG and the GFA format seem to be rather extensive, 
the idea behind the GML format is to create a simpler way of encoding graph data, 
while not falling into the pitfalls of the Bubble Format and limiting the scope of 
the graphs which could be expressed too much. 
As can be seen with the success of FASTA files for sequential data, 
the simplicity of a data format is rather important, as it makes it more likely 
for future tools to be designed with inbuilt support for the format.

The aim of the GML format is also to not unnecessarily complicate the process of 
making existing tools of the analysis pipeline 
compatible with graph data. 
Therefore, the general structure of a GML-formatted file is similar to the general structure 
of a FASTA file, which is already widely used. \\
Namely, a GML file can contain comments and data, with different blocks of data being separated 
by comments. A comment in turn consists of the character “>” to indicate the start of a comment, 
the name of the following data block, followed by a space character and any free text that 
can be used as is deemed necessary when the file is created. 
If a GML file contains no comments at all, 
then all the rows are simply interpreted as one contiguous data block.

% TODO :: create general figure of GML file:
%

Within a data block, a genomic graph is encoded in two different parts. \\
The first part is referred to as the “main path.” 
This is simply the sequence of labels on any one path from the start node 
to the end node. The start and end nodes are labelled with a hashtag symbol and a dollarsign, respectively. 
These labels are not included in the main path within the file, as they are implicitly assumed 
to exist for any such graph. \\
The second part of a data block is optional. 
If the second part is given, then the first and second part of the data block are separated by a pipe character. 
This second part is an array of info blocks, separated from each other by semicolons. \\
Finally, each info block consists of exactly four parts which are separated from each other by commas.
\begin{itemize}
\item The first part is the identifier of the path, which can contain letters and underscores, 
as well as numbers in any position but the first. 
The identifier can also be left empty.
\item The second part is the origin of the path, 
containing the identifier of the path on which this one 
originates followed by a colon and the position within that path 
at which it originates. \\
The identifier of the main path is \texttt{mp}, but in the special case of the main path the 
identifier and the colon can be left out together, e.g. \texttt{mp:8} or just \texttt{8} for position eight 
on the main path, but \texttt{path9:8} for position eight on a path with the identifier \texttt{path9}.
Identifiers need to be defined before they can be used, that 
is, \texttt{AC|a,1,G,2;,a:0,C,3} is valid, while \texttt{AC|,a:0,C,3;a,1,G,2} is not valid. \\
The counting of the position starts at number zero for the first symbol. 
However, the main path implicitly contains the hash tag symbol and the dollar sign symbol. 
Therefore the hash tag symbol on the main path is \texttt{mp:0} and 
the first alphabetical character on the main path is \texttt{mp:1}, 
while the first alphabetical character on a path with the identifier \texttt{path9} is \texttt{path9:0}.
\item The third part is the content of the path, meaning the sequence of labels of nodes on the path. 
It can be empty if the path consists of just an edge from the origin to the target without containing any nodes.
\item The fourth part is the target of the path, 
specified according to the same format as the origin of the path in the second part of the info block.
\end{itemize}

% TODO :: create figure of GML file:
% e.g. GACG|p1,1,TGG,3;,p1:0,C,p1:2 or bubble notation G(A|T(G|C)G)CG
% mainpath|infoblock;infoblock;infoblock
% \                                    /
%             data block
%
% also have the actual graph representation of that with it on the figure =)

The GML format can encode any labelled graphs which start with a special hash tag node 
and end with a dollar sign node, as long as each node within the graph 
can be reached from the start and as long as the end can be reached from 
every node as well. These constraints are acceptable for practical use, 
as nodes that cannot be reached from the start would be ignored anyway, 
as well as leaf nodes that are not the end node. \\
Despite the potential to encode such complicated structures, it is reasonably 
simple and for very short graphs even human-readable. 
With increasing graph size the readability of a GML file without special software 
decreases though, as the info blocks containing alternative paths are quite not 
located close to where they are found within the main row, but are all concentrated at the end of the file.

\subsection{FFX Format}

In the process of creating GML, we finally designed one more data format for 
genomic graphs. The idea behind FFX, the Flat Fused XBW format, is to enable saving 
a flat XBW table directly, without having to convert it to some other format 
first. \\
As shown in subsection~\ref{sec:fusing_instead_of_merging}, 
it can be helpful to fuse several separate graphs together 
instead of merging them completely. Therefore, FFX is designed 
to specifically accommodate for these kinds of fused structures as well. 
It does not however impose these fused structures on the user, 
as non-fused graphs can simply be stored as single data blocks within 
an FFX file.

The basic structure of an FFX file is again inspired by FASTA, 
containing an optional starting comment which contains an identifier for 
the contents of the file and other text that can be used freely. 
The optional comment is then followed by one or several data blocks, 
with each data block being separated by comment lines. \\
However, if several data blocks are included within one FFX file, 
they are not seen as separate entities. Instead, the program working on 
the FFX file is supposed to handle them as fused flat XBW tables, 
with the end node of the first table leading into the start node of the 
second table, the end node of the second leading into the start node of the 
third, and so on. \\
As more complex formats are less likely to become 
used by the community at large as it would be harder to re-implement them 
in various situations, certain rules are imposed on these fused data structures. 
First of all, a data block is always fused to the one directly following 
within the file. 
Theoretically, it would be possible to create a set of rules for the comment 
lines to indicate that other relations should instead take place, such as the 
end node of the first table leading to the start node of the fourth and so on, 
but even though this would increase the flexibility of the format, it would only 
increase the difficulty of writing an implementation for it. 
Likewise, each data block is only allowed to contain exactly one hash tag node 
as start and exactly one dollar sign node as end. Therefore, the path from one 
data block to the next is always completely linear, and every path through the 
entire FFX file must use every edge between the data blocks exactly once. \\
At the first glance, this might seem like a rather limiting constraint. 
However, it should be noted that much more complex behaviour is very possible 
within each data block, which can encode a complex a graph as is necessary. 
Also, this is inspired by the actual needs for a huge graph reference, 
in which the overall structure is very linear and all perturbations are on a 
rather local scale.

In FFX, each data block consists of the BWT, the $ M $ and $ F $ bit vectors, 
and additional data which could be constructed on the fly from just the BWT, 
but which takes up a lot of time to be constructed while only take up a small 
amount of space, such that saving it explicitly within the file means that the 
lengthy process of computing it again and again does not need to be executed on 
every opening of the file.

% TODO :: make clearer HOW the BWT, M and F are encoded in a data block, and WHAT other data there is
% TODO :: add pictures of FFX files and the corresponding graphs (also a picture of two graphs being merged, leading to one data block, and two graphs being fused, leading to two data blocks)

\subsection{Implementation}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                METHODS: Python Scripts / Data Format Implementations
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To compare the different data formats that have been proposed, 
we wrote several scripts in Python. 
These scripts support simplified versions of each step in a complete read alignment pipeline. 
The goal here was not to achieve a software package that could compete with 
the already commonly used alignment solutions, but rather to have a 
simple test bed for trying out different algorithmic approaches and file formats.

Section~\ref{sec:results_data_formats} contains the insights gained from 
implementing the different data formats.

\section{Graph Merging Library}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                       METHODS: Graph Merging Library
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After gathering experience on how to implement a pipeline used for aligning reads to a reference in general, 
we created the Graph Merging Library. 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{evo_gml_3.png}
\caption[GUI of the Graph Merging Library]{Graphical User Interface of the Graph Merging Library.} \label{fig:evo_gml_3}
\end{figure}
This is a software library written in JavaScript, with a graphical user interface written in HTML and CSS, 
that can be used to more explicitly understand how various algorithms for working with genomic graphs work. 
A screenshot of user interface can be seen in figure~\ref{fig:evo_gml_3}. \\
In particular, GML includes different methods for merging and fusing genomic graphs, 
which may be helpful to other future projects.

\section{Core Assumptions of GML}
\label{sec:gml_core_assumptions}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                     METHODS: Core Assumptions of GML
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

There are several core assumptions which graphs need to fulfill to be used within the Graph Merging Library. 
These are chosen to simplify working on the graph as opposed to working on more general arbitrary graphs, 
while allowing for enough freedom to encode actual real world data. 
In particular, graphs considered in GML are finite automata.

\subsection{Graph Properties}

A graph $ G = (V, E) $ used in the Graph Merging Library is generally considered to consist of labelled nodes 
and directed edges, as introduced in section~\ref{sec:graph_definition}. \\
We assume that the graph is \textit{connected}. 
% GACGT|,2,T,4;,3,,5 vs. ACCTG|,1,,5;,1,,3 with no start and end nodes, and taking out the edge in the middle
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_assume_connectedness.pdf}
\caption[Graph which is not connected]{A graph which is not connected, consisting of two clusters of nodes. The nodes within each cluster are connected among each other through edges, but are not connected to nodes of the other cluster.} \label{fig:evo_fig_assume_connectedness}
\end{figure}
We define a graph to be connected if every node of the graph is connected to every other node of the graph through some edges, 
even if not every node is reachable from every other node. This means that every 
set of two nodes needs to be connected by a series of edges, but not all of them need to face in the same direction. 
An example for a graph violating this assumption can be seen in figure~\ref{fig:evo_fig_assume_connectedness}. 
Also, the edges in the graph need to be \textit{unique}. 
% GACGTACCT|,3,,5;,6,,8 to get the arrows and then also 
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_assume_unique_edges.pdf}
\caption[Graph with edges which are not unique]{A graph with edges which are not all unique, as there are several edges which share the same origin and target node.} \label{fig:evo_fig_assume_unique_edges}
\end{figure}
This means that no two directed edges $ (u_1, v_1) {\: \in \:} E $ and $ (u_2, v_2) {\: \in \:} E $ are allowed to 
exist within the same graph for which both $ u_1 = u_2 $ and $ v_1 = v_2 $ are true. 
A graph not obliging with this requirement is shown in figure~\ref{fig:evo_fig_assume_unique_edges}. \\
Graphs within the Graph Merging Library are supposed to be \textit{acyclic}, 
meaning that they are not allowed to contain any loops. These are paths along 
the directed edges which lead from a node onto itself, either directly or along a sequence of edges through other nodes. \\
% TODO FIGFIGFIG :: inifinite loops
The alphabet $ \Sigma $ is assumed to contain only upper case English letters for graphs within GML.

Graphs are also assumed to have exactly one node with the label “\$” which 
is the start of each path traversing the entire graph. This means that there is only one node with no incoming 
edges, this node has the label “\$”, and it is the only node in the graph with this label. 
The lexicographic value of the character “\$” is supposed to be below the 
lexicographic value of all characters in the alphabet $ \Sigma $, 
which is the case for “\$” if the alphabet only contains English letters and the encoding in 
which string characters are compared is ASCII \citep{ASCII}. \\
Furthermore, each graph is assumed to have exactly one node with the label “\textbf{\#}” in which 
each path traversing the entire graph ends. This can be reformulated as there being only one node without any 
outgoing edges, this node having the label “\textbf{\#}”, and no other node in the graph having this label. 
The lexicographic value of the character “\textbf{\#}” is supposed to be above the 
lexicographic value of all characters in the alphabet $ \Sigma $, 
which is however not the case for “\textbf{\#}” in ASCII. 
We therefore internally use the character “{\textasciicircum}”, whose lexicographic value encoded in ASCII 
is above all English letters in upper case. 
This is also the reason why we work with upper case letters as node labels. 
Before being displayed, the character “{\textasciicircum}” is replaced by “\textbf{\#}”. \\
It should be noted that we use two different conventions concerning 
an edge from the end node $ v_{\$} $ to the start node $ v_{\textbf{\#}} $. 
When we are working with XBW node tables or with flat XBW tables, 
we assume that there is an edge leading from the end node to the start node. 
This makes working with these tables slightly easier, as the FC row corresponds 
exactly to the alphabetically sorted BWT row. 
If there was no such edge from $ v_{\$} $ to $ v_{\textbf{\#}} $, 
then the BWT would contain the symbol “\textbf{\#}”, but it would be missing the “\$” character. 
Also, the FC row would contain the “\$” character, but it would be missing the “\textbf{\#}”. \\
On the other hand, when we are considering graphs internally as arrays of nodes which 
each contain a label, an array of predecessors and an array of successors, 
then we explicitly assume there not to be an edge from the end node to the start node. 
This means that the predecessor array of the start node is empty, 
and the successor array of the end node is empty, 
which can be checked as alternative to reading out the label of the node and checking 
whether it is a special character.

% AtctCAG|p1,1,gg,4;PATH,2,cta,6;P2,5,T,7;,P1:1,,P2:0
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_assume_all_good.pdf}
\caption[Graph fulfilling core assumptions of GML]{A graph fulfilling the core assumptions of GML.} \label{fig:evo_fig_assume_all_good}
\end{figure}

A graph which is not violating any of aforementioned assumptions can be seen in figure~\ref{fig:evo_fig_assume_all_good}.

\subsection{Additional Assumptions for Merging Graphs}

If graphs are supposed to be merged together, then another core assumption is added, 
which states that no node splitting is allowed to be needed between these graphs. 
That is, at no point within the merging process shall it be necessary to 
connect the two graphs through more than one edge.
% informally, we refer to this scenario as "splitover"

% TODO FIGFIGFIG :: show an image of a splitover - several edges connecting this and that, etc. bla blubb

\section{Generating a Genomic Graph}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                  METHODS: Generating a Genomic Graph
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To use a genomic graph in the Graph Merging Library, 
a GML file can be opened which contains the desired graph, 
a valid GML data block can be entered directly into an input field, 
or a graph can be created randomly for testing purposes.

\subsection{Randomly Generating New Graphs}

The Graph Merging Library contains the function \texttt{generateRandomGraphString} for 
the automatic generation of random graphs, which can be used to quickly generate input for testing the 
provided merging functions. 
This function can be called without any arguments, in which case it will generate short graphs 
or strings. If necessary, the length of the main row can however also be provided to generate 
graphs of different lengths. The alphabet can also be specified, which otherwise will be 
set to $ \Sigma = \{ \textrm{A, C, G, T} \} $. Finally, two thresholds with values between $ 0\% $ and $ 100\% $ can 
be specified, which tell the function how likely it is to add at least one infoblock to the graph, 
and how likely it is to add more infoblocks after adding the first.

% TODO :: add more text

\subsection{Internal Graph Format}
\label{sec:internal_graph_format}

When a GML file is being opened, then some work still needs to be undertaken 
to convert the opened graph into the extended XBW format, and the graph is stored 
in a different format internally at first before this conversion occurs. \\
That is, each graph is stored as array which contains all nodes as elements. 
Every node here is an object which has the properties \texttt{c}, \texttt{n}, \texttt{p} and 
possibly \texttt{f}. The \texttt{c} property contains one character, which is the label of the node. 
The \texttt{f} property, if it is defined, contains the prefix of the node. Until the prefixes have 
been calculated, these properties can be missing. \\
The \texttt{n} property contains an array of integers, with each integer being the index 
of a node in the array representing the graph. It represents a list of all succeeding nodes. 
Similarly, the \texttt{p} property is a list of preceding nodes, implemented as array of integers, 
with each integer again being the index of a preceding node in the graph array. \\
The start node with label “\textbf{\#}” is assumed to be found as the first element 
of the graph array. Apart from this node, the positions of the other nodes within 
the array are not significant. In fact, elements of the array are also allowed to 
be empty or the boolean value \texttt{false}. This is helpful when deleting 
nodes from the graph, as the nodes with higher indices therefore do not need to be re-indexed, 
which would entail a lot of work as then all the arrays of preceding and succeeding nodes 
also would need to be updated accordingly. 
Therefore, every loop iterating over the nodes of the graph needs to check for the 
current node to actually exist before performing any work on it.

The extended XBW format requires for each graph for start with a node labelled with a 
hash tag sign and to end with a node labelled with a dollar sign, 
but these nodes are not explicitly encoded in the GML format.
Therefore, the extra node with the \textbf{\#} label is created first when opening a GML file. 
The main row as specified in the GML file is then added as one possible way 
of traversing the graph from the \textbf{\#} node to the \$ node, which is added at the end of the main row. \\
Finally, the info blocks are iterated over, and their contents are added as edges and nodes which are part 
of paths off the main row.
% TODO :: add algorithm: The entire process is illustrated in algorithm~\ref{alg:...}.

\section{Sanitizing the Graph}

If graphs which are used as inputs for the GML merging conform to the assumptions 
stated in section~\ref{sec:gml_core_assumptions}, 
then they can be worked on and results can be given out. 
However, in case of inputs not conforming to these assumptions, the results of the GML algorithms 
are not useful or even no results are given out, as infinite loops are encountered. 
Therefore, a sanitization step is used to ensure that the input actually conforms to the 
stated assumptions. If this is not the case, which can happen for randomly created input graphs, 
then a warning is given out telling the user that invalid input was encountered and 
no attempt is being made to actually perform work on the graph.

To perform this check of the core assumptions, the function \texttt{sanitizeAutomaton} is called within GML 
whenever an input graph string is newly converted to an internal automaton data structure representing it. 
It in turn calls several functions which check one assumption at a time. \\
The first one of these functions is called \texttt{eliminateMultipleEdgesInAutomaton}. 
Its underlying functionality can be found in algorithm~\ref{alg:eliminateMultipleEdgesInAutomaton}.
\begin{algorithm}
\caption[Unify multiple edges in a graph]{Unify multiple edges in a graph.}
\label{alg:eliminateMultipleEdgesInAutomaton}
\begin{algorithmic}[1]
\ForAll {graph.nodes \textbf{as} node}
	\If {$ \textrm{\textbf{length of} node.predecessors} > 1 $}
	\Comment{Unify array of preceding nodes}
		\State $ \textrm{new\_array} \gets [ \, ] $
		\ForAll {node.predecessors \textbf{as} predecessor}
			\If {\textbf{not} ( new\_array \textbf{contains} predecessor )}
				\State \textbf{append} predecessor \textbf{to} new\_array
			\EndIf
		\EndFor
		\State $ \textrm{node.predecessors} \gets \textrm{new\_array} $
	\EndIf
	
	\If {$ \textrm{\textbf{length of} node.successors} > 1 $}
	\Comment{Unify array of succeeding nodes}
		\State $ \textrm{new\_array} \gets [ \, ] $
		\ForAll {node.successors \textbf{as} successor}
			\If {\textbf{not} ( new\_array \textbf{contains} successor )}
				\State \textbf{append} successor \textbf{to} new\_array
			\EndIf
		\EndFor
		\State $ \textrm{node.successors} \gets \textrm{new\_array} $
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
It ensures that each edge is unique, meaning that there is no pair of 
edges $ (u_1, v_1) {\: \in \:} E $ and $ (u_2, v_2) {\: \in \:} E $ for which both $ u_1 = u_2 $ and $ v_1 = v_2 $ are true. 
It does not actually throw an error if such edges are encountered, but simply deletes one edge of the pair, until none 
of these pairs of edges are left. \\
The next function is \texttt{checkAutomatonForLoops}. 
Its general approach is shown in algorithm~\ref{alg:checkAutomatonForLoops}.
\begin{algorithm}
\caption[Check if graph contains loops]{Check if a graph contains loops.}
\label{alg:checkAutomatonForLoops}
\begin{algorithmic}[1]
\State $ \textrm{paths} \gets [ \, [ \, 0 \, ] \, ] $
\While {$ \textrm{\textbf{length of} paths} > 0 $}
	\ForAll {paths \textbf{as} path}
		\State $ \textrm{current\_node} \gets \textrm{\textbf{last element of} path} $
		\If {$ \textrm{\textbf{length of} current\_node.successors} > 0 $}
			\ForAll {current\_node.successors \textbf{as} successor}
				\If {path \textbf{contains} successor}
					\State \Return true
					\Comment{Same node encountered twice}
				\EndIf

				\If {\textbf{last iteration within this for loop}}
					\State \textbf{append} successor \textbf{to} path
				\Else
					\State $ \textrm{next\_path} \gets \textrm{path} $
					\State \textbf{append} successor \textbf{to} next\_path
					\State \textbf{append} next\_path \textbf{to} paths
				\EndIf
			\EndFor
		\Else
			\State \textbf{delete} path \textbf{from} paths
			\Comment{Drop the path if \$ node was reached}
		\EndIf
	\EndFor
\EndWhile
\State \Return false
\end{algorithmic}
\end{algorithm}
This function traverses all paths through the graph, starting from the start node, 
and checks for each new node added to a path if that node has already been traversed within the path before. 
If such a node is encountered which is traversed several times within one path, then an 
error is given out and the invalid input warning is displayed to the user. \\
Finally, the function \texttt{checkAutomatonForIncorrectChars} as shown in 
automaton~\ref{alg:checkAutomatonForIncorrectChars} iterates over all nodes of the automaton 
\begin{algorithm}
\caption[Check if graph labels contain invalid characters]{Check if graph labels contain invalid characters.}
\label{alg:checkAutomatonForIncorrectChars}
\begin{algorithmic}[1]
\ForAll {graph.nodes \textbf{as} node}
	\If {( node.label {\boldmath$ = $} \$ ) \textbf{and} ( \textbf{length of} node.successors {\boldmath$ = $} 0 ) }
		\State \textbf{continue}
	\EndIf
	
	\If {( node.label {\boldmath$ = $} \textbf{\#} ) \textbf{and} ( \textbf{length of} node.predecessors {\boldmath$ = $} 0 ) }
		\State \textbf{continue}
	\EndIf
	
	\If {( $ \textrm{node.label \textbf{in}} \, [ \textrm{A \textbf{to} Z} ] $ ) and ( \textbf{length of} node.label {\boldmath$ = $} 1 )}
		\State \textbf{continue}
	\EndIf
	
	\State \Return true
\EndFor
\State \Return false
\end{algorithmic}
\end{algorithm}
and checks if the labels conform to the assumptions, meaning that every node needs to have a 
label with exactly one English upper case letter, except for the start node which has the label “\$” and 
the end node which has the label “\textbf{\#}”. 
Again, if a node is encountered which does not conform to these assumptions, then the invalid input message 
is shown to the user and the computation is stopped. \\
We could also use some function to check if the graph is connected, 
but this is actually not necessary, as the GML format in which we open the graph can only encode connected 
graphs anyway. The reason for this is that a graph encoded in GML is encoded as a main path, 
together with other paths originating and terminating on the already existing paths. 
Therefore, no path can be encoded which is separate from the others, and checking afterwards if 
the emerging graph is connected becomes unnecessary.
% TODO :: add algorithms for all these!

\section{Visualizing the Graph}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                       METHODS: Visualizing the Graph
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After opening the graph, it can be helpful to show it to the user for 
quick and easy visual inspection. 
We therefore included a small visualization function within the Graph Merging Library, 
which is capable of displaying simple graphs nicely. 
Algorithm~\ref{alg:visualize} shows how this function works in general. 
\begin{algorithm}
\caption[Visualize a graph]{Visualizes a graph by first displaying one path from 
the starting node to the end node, referred to as main row, 
and then adding alternative paths around the established core.}
\label{alg:visualize}
\begin{algorithmic}[1]
\State $ \textrm{current\_node} \gets \textrm{graph}[0] $
\Comment{Display main row}
\State \textbf{draw} current\_node
\State $ \textrm{more\_nodes} \gets [ \, ] $
\While {current\_node.label \textbf{is not} \$}
	\State \textbf{append} current\_node \textbf{to} more\_nodes
	\State $ \textrm{current\_node} \gets \textrm{current\_node.successors}[0] $
	\State \textbf{draw} current\_node
\EndWhile
\State \phantom{emptyline}
\ForAll {more\_nodes \textbf{as} path \$}
\Comment{Display other paths}
	\State $ \textrm{current\_node} \gets \textrm{path} $
	\If {current\_node has not been drawn}
		\State $ \textrm{node} \gets \textrm{current\_node} $
		\State $ \textrm{path} \gets [ \, ] $
		\While {node has not been drawn}
			\State \textbf{append} node \textbf{to} path
			\State $ \textrm{node} \gets \textrm{node.successors}[0] $
			\If {node has not been in more\_nodes before}
				\State \textbf{append} node \textbf{to} more\_nodes
			\EndIf
		\EndWhile
		\State \textbf{draw} nodes \textbf{in} path together
	\EndIf
	\ForAll {current\_node.predecessors \textbf{as} predecessor}
		\State \textbf{draw} edge \textbf{from} predecessor \textbf{to} current\_node
	\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}
Having a closer look at line 1 of the algorithm, 
it can be seen that it constructs the main row 
by starting in the first node, which is assumed to 
be labelled with the hash tag symbol. 
In line 6 of the algorithm it can then be seen 
that to construct the main row starting with this node, 
the algorithm always jumps into the first of the successors of the current node. \\
Therefore, before a graph can be passed to the visualization function, 
it needs to be ensured that its hash tag node is stored in the very first position 
and that the node labelled with the dollar sign can be reached by always travelling 
along the first of the successors. 
A separate function called \texttt{makeAutomatonPretty} is used to ensure that this is always the case, 
by traversing all possible paths and picking one path which leads to the dollar sign node. 
In the options it can be chosen whether this function 
should find one of the shortest paths, which is faster, or one of the longest paths, 
which leads to a better visualization. 
When it is instructed to find one of the longest paths, it will however still ignore 
paths including loops, as it might otherwise be trapped in the loop indefinitely, 
trying to find yet longer paths. 
An example for how this choice affects 
the visualized graph can be seen in figure~\ref{fig:evo_fig_visualize_short_vs_long}.  
% merge GACGT|,2,T,4;,3,,5 vs. ACCT|,1,,4;,1,,3, once with normal settings, once with forcing shortest path
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_visualize_short_vs_long.pdf}
\caption[Visualizations of graph with shortest and longest main rows]{Two visualizations of the same graph. The visualization on the top uses the shortest possible path as main row, while the visualization on the bottom uses the longest possible path.} \label{fig:evo_fig_visualize_short_vs_long}
\end{figure}
The graph nodes are then adjusted in such a way that following the first successor of each 
node upon starting at the hash tag node leads directly to the dollar sign node. \\
It is also notable how exactly the iteration over the nodes works out. 
In line 10 we iterate over all nodes that have been added to more\_nodes, 
an array which keeps track of nodes which we have not fully worked on yet. 
This means that we may have already drawn the node, but we have not yet 
drawn all the incoming edges, and we may possibly not even have drawn the node itself. 
The loop in line 15 then iterates over all the first successors of that node, 
and adds them to an array called path, until a node is encountered that has already been drawn. 
Finally, in line 22, the entire path is drawn together. \\
Algorithmically it would be much more straightforward to simply iterate over all nodes 
and draw them when we encounter them, but in that case we would not have a good idea about 
where it might make sense to draw them. The approach of iterating over nodes 
that have not been worked on and nesting into this an iteration of the successors of that node 
allows us to instead draw entire parts of paths at once, so that we have a good idea 
where exactly to put the nodes on the display when drawing them, as they can simply 
be arranged next to each other.

\section{Direct Graph Merging}
\label{sec:direct_graph_merging}

The aim of the Graph Merging Library is to provide advanced algorithms 
for the direct merging of flat XBW tables. 
We therefore developed the function \texttt{mergeAutomata} to merge two uncompressed input graphs, 
so that the results of the advanced merging 
algorithms can be checked against the results of this robust and simple method.

% ...

\section{Ensuring the Graph is Reverse Deterministic}
\label{sec:why_rev_det}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                 METHODS: Ensuring the Graph is Reverse Deterministic
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before being able to encode the graph as either XBW node table or as flat XBW table, 
it should be ensured that it actually is reverse deterministic.

For any node within the graph, we can construct a prefix, 
which is the sequence of its own label concatenated with the labels of the nodes which are 
traversed when exiting that node. 
% TODO :: insert image about prefixes and how they are generated 
% (even through strands that are split apart, but have identical labels)
The extended XBW compression scheme for graphs as proposed by \citet{Siren2014} relies 
on the possibility of ordering all nodes of a graph alphabetically, based on their prefixes. 
After opening a file format that can contain arbitrary graphs, 
it is therefore first of all necessary to ensure the graph is reverse deterministic, 
as otherwise at least two nodes could not be unambiguously sorted against each other. 
File formats which can contain graphs which are not reverse deterministic include 
FASTG, GFA, GML and the bubble format, while the FFX format guarantees that 
the stored data is reverse deterministic as its entire encoding paradigm 
otherwise would not work. \\
% TODO :: insert figure in which we can see a non-reverse deterministic graph and the prefixes being the same.
To understand why a graph not being reverse deterministic would lead to 
at least two nodes not being able to be unambiguously sorted by their prefixes, 
we can recall the definition of a reverse deterministic graph:

% \begin{moyadef}
\textbf{Definition} A graph is \textit{reverse deterministic} if and only if each node has no two predecessors with the same label.
% \end{moyadef}

% What we want to prove is the following theorem.
% \begin{moyatheorem}
% If a graph is not reverse deterministic, 
% then at least two of its nodes cannot be unambiguously sorted against each other.
% \end{moyatheorem}
% \begin{proof}
From the definition it can be seen that a graph $ G = (V, E) $ not being reverse deterministic means that there 
is a node $ u {\: \in \:} V $ which has at least two predecessors $ v $ and $ w $ which share a label. 
As $ v $ and $ w $ share a label, their prefixes both start with the same label. \\
Without loss of generality we can now focus on one of the two nodes $ v $ and $ w $. 
As $ v $ is a predecessor of $ u $, we can see that the 
prefix of $ v $ either continues with the prefix of $ u $, 
or that it will not be able to continue after the first character 
as there is another successor of $ v $ which has a different label than $ u $ does. \\
The same holds true for $ w $, such that both $ v $ and $ w $ have labels starting 
with the same character and are of length 1 unless they are that character concatenated 
with the prefix of $ u $. \\
Now, in case of one of the prefixes just having length 1, we cannot sort $ v $ and $ w $ 
unambiguously, as their prefixes are the same for all given characters. 
On the other hand, if both prefixes have length above 1, then they are both 
the same character concatenated with the prefix of $ u $, as we can again not sort $ v $ and $ w $ 
unambiguously. 
Therefore, a reverse deterministic graph invalidates the assumption that we can sort 
all of its nodes unambiguously alphabetically by its prefixes. \qed
% \end{proof}

The program therefore first needs to check if the opened graph is reverse deterministic. 
This can easily done by comparing the labels of every nodes' predecessors, as can be seen 
in algorithm~\ref{alg:isAutomatonReverseDeterministic}.

\begin{algorithm}
\caption[Check if a graph is reverse deterministic]{Checks if a graph is reverse deterministic.}
\label{alg:isAutomatonReverseDeterministic}
\begin{algorithmic}[1]
\ForAll {graph.nodes \textbf{as} node}
	\State $ \textrm{encountered\_characters} \gets [ \, ] $
	\ForAll {node.predecessors \textbf{as} predecessor}
		\If {encountered\_characters \textbf{contains} predecessor.label}
			\State \Return false
		\Else
			\State \textbf{append} predecessor.label \textbf{to} encountered\_characters
		\EndIf
	\EndFor
\EndFor
\State \Return true
\end{algorithmic}
\end{algorithm}

If the graph is found not to be reverse deterministic, then it needs to be adjusted 
until it becomes reverse deterministic. 
It is hereby important that any change to the graph does not change the language 
that the graph realises, which represents all genomic strings that are encoded within the graph. \\
To change the graph in this way, three different operations can be used on the graph. 
These three operations are to merge two nodes, 
to move an edge from one pair of nodes to another pair, 
and to split a node. 
Of course, not all nodes can be merged with each other and not all edges can be moved around 
without changing the language that the graph represents, which is why a special algorithm needs 
to determine which operation to choose next and how to apply it.

% TODO :: write more about that algorithm, basically giving us:
% makeAutomatonReverseDeterministic
% makeAutomatonReverseDeterministic_int
% reverseDeterminizer
% mergeNodesInAutomaton
% splitNodeInAutomaton

\section{Prefix Sorting}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                              METHODS: Prefix Sorting
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next step is to ensure that all the nodes can actually be sorted by their prefixes. 
If this requirement is fulfilled, then we call the graph prefix sorted. \\
The graph is already reverse deterministic, but that does not mean that all the nodes 
necessarily have unique prefixes which make them unambiguously sortable. 
An example for a graph which is reverse deterministic but contains several nodes 
with the same prefixes can be seen in figure~\ref{fig:evo_gml_rev_det_but_not_prev_sort}. 
% CGTACGTAA|,2,A,4;,6,G,8 - do not alternate sides; print first graph with prefixes
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_gml_rev_det_but_not_prev_sort.pdf}
\caption[Reverse deterministic graph that is not prefix sorted]{Reverse deterministic graph that is not prefix sorted. Among others, the two nodes with label \textup{“C”} share the same prefix.} \label{fig:evo_gml_rev_det_but_not_prev_sort}
\end{figure}
If none of the nodes in the graph had more than one outgoing edge, then the graph 
would already be prefix sorted, as every prefix would end with the dollar sign 
and have this dollar sign in a different position than any of the other prefixes would. 
However, as the nodes in the graph we consider can actually have several outgoing edges, 
it is possible that they lead to nodes which have different labels. In this case, 
we write a special character such as the exclamation point at the character in the prefix, 
to note that this character can not be unambiguously determined. \\
The process of achieving a prefix sorted graph is therefore based on iterating through 
the nodes and reducing the amount of 
prefixes which end on exclamation points, as can be seen in algorithm~\ref{alg:workOnAutomatonPrefixes}. 
\begin{algorithm}
\caption[Prefix sort a graph]{Prefix sorting a graph by splitting nodes with prefixes that are not unambiguously sortable.}
\label{alg:workOnAutomatonPrefixes}
\begin{algorithmic}[1]
\State $ \textrm{graph\_is\_not\_prefix\_sorted} \gets \texttt{true} $
\While{graph\_is\_not\_prefix\_sorted}
	\State $ \textrm{graph\_is\_not\_prefix\_sorted} \gets \texttt{false} $
	\ForAll {this\_node \textbf{in} graph}
		\State $ \textrm{this\_node.prefix} \gets \textrm{this\_node.label} $
	\EndFor
	\ForAll {this\_node \textbf{in} graph}
	\Comment {Check for all nodes...}
		\State $ \textrm{same\_as} \gets [ \, ] $
		\ForAll {that\_node \textbf{in} graph}
		\Comment {... if any node has the same prefix}
			\If {this\_node.prefix {\boldmath$ = $} that\_node.prefix}
				\State \textbf{append} $ [\textrm{that\_node}] $ \textbf{to} same\_as
			\EndIf
		\EndFor
		\If {\textbf{length of} $ \textrm{same\_as} > 1 $}
			\ForAll {node \textbf{in} same\_as}
			\Comment {Iterate over nodes with the same prefix}
				\State $ \textrm{first\_node\_label} \gets \textrm{next character to append to node.prefix} $
				\If {first\_node\_label cannot be found unambiguously}
					\State split the last node whose label was added to the prefix and which
						\State \phantom{first} has more than one successor
					\State $ \textrm{graph\_is\_not\_prefix\_sorted} \gets \texttt{true} $
				\Else
					\State \textbf{append} first\_node\_label \textbf{to} node.prefix
				\EndIf
			\EndFor
		\EndIf
	\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}
In line 18 of this algorithm, a node with more than one successor is split into several nodes. 
Such a node needs to exist there, as we reach this point in the algorithm due to a prefix 
not being unambiguously constructable, and the only way for this to happen is if a node has 
several successors with different prefixes. Therefore, we must be able to find a node with 
several successors for which splitting it enables us to build longer prefixes. \\
The exact approach for splitting a node $ N $ is to replace $ N $ by as many new nodes as $ N $ has 
outgoing edges. 
Every one of the nodes replacing $ N $ has exactly one outgoing edge, leading to 
one of the successors of $ N $. 
In this fashion, every successor of $ N $ obtains one of the nodes replacing $ N $ as predecessors. 
Every one of the new nodes also obtains incoming edges to all the nodes that are predecessors of $ N $. 
Finally, every one of the nodes replacing $ N $ is given the label of $ N $. 
The process of splitting a node is shown in figure~\ref{fig:evo_fig_node_splitting}. \\
% GACCTAATG|,5,C,8;,1,AG,5 - here we did a bit of work manually; this is about the splitting of the T node,
% but the position looks better after the splitting of the C node, so we copied some arrows manually in inkscape etc.
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_fig_node_splitting.pdf}
\caption[Node splitting example]{Node splitting example. On the top, the graph is visualized before the splitting of the central node with label \textup{“T”} and prefix \textup{“T!”}. On the bottom, the same graph can be seen after node splitting occurred. The node has been replaced with one node with prefix \textup{“TC”} and one node with prefix \textup{“TA”}. Both nodes have all the incoming edges that the original node had.} \label{fig:evo_fig_node_splitting}
\end{figure}
It should be noted that we do not wish to remove all exclamation points entirely, 
as it is enough for a prefix to be so long as to be unambiguously sortable against all 
other prefixes of nodes in the graph. If a prefix is long enough to ensure this, then 
it is completely irrelevant whether it eventually ends in an exclamation point or in a dollar sign.

\section{Creating a Node Table}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                       METHODS: Creating a Node Table
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Ultimately, we want to generate a flat XBW table from the prefix sorted graph, 
and explore methods for merging these flat XBW tables. 
However, in this section we first consider node tables, which share several characteristics 
with flat XBW tables. 
Node tables are easier to understand and to use as they are more closely related to the 
underlying graph, which means that the visualized graph can be used to understand their behaviour. 
The definition of a node table can be found in section~\ref{sec:node_table_definition}.

% To create a node table when a prefix sorted graph is given, we can ...
%
% ...

\section{Working on a Node Table}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                     METHODS: Working on a Node Table
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Assuming that we have created a node table encoding a genomic graph, 
we can perform various operations on it. 
This includes going from one node in the table to its successors or predecessors, 
as well as reading out the prefix of a given node to a certain length.

\subsection{Navigating Through the Graph and Reading Out Labels}

Assuming that we are interested in a certain column $ i $ of the node table, 
which corresponds to a node $ v_i $ in the graph represented by the table, 
then we can read out the label $ l ( v_i ) $ of that node by reading out the prefix 
from the table and taking the first character of this prefix.

To read out the label of a succeeding node, we actually have several possibilities. 
One way is to take the second character of the prefix of the current node, 
which corresponds to the first character of the prefix of each succeeding node, 
and therefore to their labels. This however only works if the prefix of $ v_i $ is 
given in the table with a length of more than one character, and if all succeeding nodes 
of $ v_i $ have the same label, as the prefix of $ v_i $ will otherwise have the 
error character “!” in the second position. \\
A similar approach can be used to read out the labels of the preceding nodes of $ v_i $. 
All these labels are different, as the graph has been made reverse deterministic, 
and they are stored in the BWT row of the node table. So if we find $ \textrm{BWT} ( i ) = \textrm{A{\pipe}G} $, 
then we know that $ v_i $ has two preceding nodes with the labels A and G. 
This however only allows us to read out the labels of the immediately preceding nodes, 
while not enabling us to find the labels of nodes preceding the predecessors of $ v_i $.

A more robust approach for reading out labels of preceding and succeeding nodes 
is to use navigation functions to traverse the graph from $ v_i $ to the succeeding 
or preceding node that we are interested in, and to then read out the label of that 
node as first letter of its prefix. \\
To navigate the graph in the forward direction, that is, to find the immediate successors 
of a node $ v_i $, we can use the \texttt{nextNodes} function. Algorithm~\ref{alg:nextNodes} shows how this 
function works. 
\begin{algorithm}
\caption[Find succeeding nodes in a node table]{Given a column $ i $ in an XBW node table, find the indices of the columns corresponding to the succeeding nodes.}
\label{alg:nextNodes}
\begin{algorithmic}[1]
\State $ \textrm{label} \gets \textrm{prefix} [ \, i \, ] [ \, 0 \, ] $
\State $ \textrm{jump\_over} \gets 0 $

\For {$ j $ \textbf{from} 0 \textbf{to} $ i $}
	\If {label {\boldmath$ = $} $ \textrm{prefix} [ \, j \, ] [ \, 0 \, ] $ }
		\State \textbf{increase} jump\_over
	\EndIf
\EndFor

\State $ \textrm{outdegree} \gets \textbf{length of} \, M [ \, i \, ] $
\State $ \textrm{nodes\_found} \gets [ \, ] $

\For {$ k $ \textbf{from} 0 \textbf{to length of} BWT}
	\If {$ \textrm{BWT} [ \, k \, ] $ \textbf{contains} label}
		\If {$ \textrm{jump\_over} > 0 $}
			\State \textbf{decrease} jump\_over
		\Else
			\State \textbf{decrease} outdegree
			\State \textbf{append} $ k $ \textbf{to} nodes\_found
			
			\If {$ \textrm{outdegree} < 1 $}
				\State \Return nodes\_found
			\EndIf
		\EndIf
	\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
The main idea behind this function is to add up all the lengths of the $ M $ values of 
the columns in the node table to the left of $ i $ which have the same label as $ v_i $ does. 
The resulting integer is stored in the variable jump\_over, as we intend to jump over that many 
nodes in the next step. 
We then iterate over the node table once more, this time reducing the amount of jump\_over by one for 
each column which contains the label of $ v_i $ in its BWT cell. 
When jump\_over reaches zero, we start adding the indices of the columns which we find to 
the output, until we have added as many indices as the outdegree of $ v_i $. \\
This approach works based on the fact that the left-most occurrence of a letter in the labels of the nodes corresponds 
to the left-most occurrence of this letter in the BWT, the second occurrence of a letter in the labels of the nodes 
corresponds to the second occurrence of this letter in the BWT, and so on.

% ... - also write something about nextNodes! (maybe not quiiite so much though? who knows!)

\subsection{Constructing Prefixes}

If we are given the index of a column in the node table, corresponding to a node in the graph, 
then we can easily read out its prefix as the content of the prefix row in the given column. 
However, we might need to construct a longer prefix. 
Such a situation could arise if we were searching for a given text of length $ n $ within the 
graph, while the prefix we get from the table only has length $ m $ with $ m < n $. 
If the prefix agrees with the text for all the characters that are given, then we want to 
append more characters to the prefix until its length reaches $ n $ and a clear decision can 
be made on whether the prefix agrees with the text or not. \\
To construct such a longer prefix given a node table with short prefixes, we can use the 
idea of \textit{prefix doubling}. 
Hereby we do not append one character to the prefix at a time by iterating over the labels of succeeding nodes, 
but instead we append the entire prefixes of succeeding nodes.

% ... TODO :: more? algorithm? At least some sort of figure illustrating this?
% (I guess a very very simple fake algorithm would actually do it here ^^)

% \subsection{Locating Strings in the Graph}
% 
% ... - add this section about how strings in the graph can be located!

\section{Merging Node Tables}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                         METHODS: Merging Node Tables
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The main aim of the Graph Merging Library is to provide algorithms 
to merge and fuse flat XBW tables. 
However, working with flat XBW tables directly is rather cumbersome, 
and it can therefore help to first investigate the simpler case 
of merging two node tables.

% Algorithm for Merging Node Tables (function GML.merge_BWTs_advanced()):
%
% find last letter of DH_1 and first letter of DH_2
% > yepp, this is truly the label of the node BEFORE $ / the label of the node AFTER #,
%   so this assumes that there is only one edge into $ and only one edge out of #!
%
% add origin to H_1 and H_2
% concatenate H_1 and H_2 as H_12
% replace $ having origin 0 with $#firstLetterH_2 within prefixes
% and more

% ...

\section{Creating a Flat Table}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                       METHODS: Creating a Flat Table
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We now wish to create a flat XBW table based on a node table. 
% TODO :: later on also create a flat XBW table directly, without the node table in between?
For the general definition of such a flat table, see section~\ref{sec:flat_table_definition}.

The process of converting an XBW node table into a flat table is illustrated in tables~\ref{table:evo_node_to_flat_node} 
and \ref{table:evo_node_to_flat_flat}, which respectively show the a node table and a flat table representing the same graph. \\
% GATAATG|,3,C,6;,1,AG,3
{
% reduce whitespace in this table (default is might be 6pt)
\renewcommand{\tabcolsep}{5pt}
\begin{table}[htb]
\centering
\caption[XBW node table before conversion to flat table]{XBW node table before conversion to flat table. 
Table~\ref{table:evo_node_to_flat_flat} shows the corresponding flat table.}
\begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | }
\hline
G & T & G & G & G & A & T & T & \textbf{\#} & A & A|G & A|G & A|C & \$ & \textbf{BWT} \\ \hline 
\$ & AA & AG & ATA & ATC & ATG & C & G\$ & GA & GT & TA & TC & TG & \textbf{\#} & \textbf{Prefix} \\ \hline 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 100 & 10 & 1 & 1 & 1 & 1 & $\boldsymbol{M}$ \\ \hline 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 10 & 10 & 10 & 1 & $\boldsymbol{F}$ \\ \hline 
\end{tabular}
\label{table:evo_node_to_flat_node}
\end{table}
}
\begin{table}[htb]
\centering
\caption[Flat XBW table after conversion from node table]{Flat XBW table after conversion from node table. 
Table~\ref{table:evo_node_to_flat_node} shows the corresponding node table.}
\begin{tabular}{ | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | c | }
\hline
G & T & G & G & G & A & T & T & \textbf{\#} & A & A & G & A & G & A & C & \$ & \textbf{BWT} \\ \hline 
\$ & A & A & A & A & A & C & G & G & G & G & G & G & T & T & T & \textbf{\#} & \textbf{FC} \\ \hline 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & $\boldsymbol{M}$ \\ \hline 
1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & $\boldsymbol{F}$ \\ \hline
\end{tabular}
\label{table:evo_node_to_flat_flat}
\end{table}
% TODO :: make certain that the previous two figures end up on the same page!
For the BWT, $ M $ and $ F $ rows, the process of achieving this conversion is exactly the same. 
We join all the cells together to achieve one string containing the entire 
row, and then split this row in between every character. 
If there were pipe characters in between some of the characters, such as 
for visualization purposes within the BWT cells, then we omit them in this process. \\
The process for the prefixes is a little bit different, as the flat XBW table does 
not contain the whole prefixes due to their immense size. 
The flat XBW table can however be thought of as containing FC, the first column data (where 
the first column refers to the first column of the alphabetically sorted cyclic rotations, 
not to the first column of this table.) 
As the FC field corresponds to the label of the node, and as the prefixes in the node table 
begin with the labels of their respective nodes, the two rows are intimately linked 
and the FC row can be constructed from the prefixes in the node table. 
For the construction of the FC row, it is also necessary to consider the amount of 
outgoing edges of a node, which is encoded in $ M $. 
In total, to generate the FC row in the flat table based on the prefix and $ M $ rows in the 
node table, we start with an empty string for FC and iterate through the prefixes in the node table. 
For each prefix, we read out the first character of that prefix. 
We then add this first character as often to FC, as the contents of the $ M $ cell 
in the corresponding column are long. So if the value of $ M $ is 100, 
then the first character of that prefix needs to be added three times to FC. \\
Having constructed the BWT, FC, $ M $ and $ F $ rows from the original node table, 
we have now created a flat XBW table which represents the same graph that was represented 
by the original table.

\section{Working on a Flat Table}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                     METHODS: Working on a Flat Table
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Before considering the rather challenging problem of merging several flat XBW tables, 
we should first investigate how work within such flat tables can be performed in general. 
In particular, it is helpful to understand how 
the functions LF and $ \Psi $ can be used to find the predecessors and successors 
of nodes, respectively.

% ...

\section{Merging Flat Tables}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                         METHODS: Merging Flat Tables
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The merging of flat XBW tables is in principle similar to the merging of node tables. 
We again iterate over all nodes of all involved graphs and construct a new table 
based on them. 
However, a big difference to the merging of the node tables is that we no longer have 
a data structure available to store the computed prefixes of all columns. 
This is intentional, as storing all of these prefixes in memory would be prohibitive 
in a real-world scenario. The implication of not having the prefixes readily available 
are that we can no longer keep track of problematic nodes that easily. 
We also cannot merge the separate tables together as they are and work on the node splitting to expand the prefixes 
after the merge, as the flat XBW format relies heavily on the ordering on the columns. 
We simply cannot guarantee that we will order the columns correctly 
if we do not have expanded prefixes in the first place. \\
We therefore decided to implement the merging of flat XBW tables in two steps, 
with the first being an iteration over all columns of the tables that are supposed to be merged, 
with the aim of determining which prefixes will need to be expanded. The corresponding nodes are 
split and the prefixes recalculated until no further node splitting will be necessary when the actual 
merge occurs. \\
The second step is then to carry out the merging. This in itself is complicated by the fact 
that in flat XBW tables, entire columns do not necessarily form a union. That is, 
in the node table, each column consisting of an entry in the BWT, a prefix, and values for $ M $ and $ F $, 
represents one node in the graph, and can not be taken apart. 
However, in the flat XBW table, column do not form such a union. 
Instead, an $ F $ value of 0 shows that the BWT of this column forms a union with the BWT of the column to the left, 
and an $ M $ value of 0 shows that the first column value of this column forms a union 
with the first column value of the column to the left.

% TODO :: add algorithm!

\section{Fusing Instead of Merging}
\label{sec:fusing_instead_of_merging}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                   METHODS: Fusing Instead of Merging
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As shown in the previous section, merging flat XBW tables is rather complex. 
It requires a lot of computation and due to a lot of node splitting 
necessary to ensure unambiguously sortable prefixes along the entire merged 
graph can result in large amounts of node splitting. This leads to an increase 
in file size as opposed to the total file size of the individual graphs that are 
merged. \\
To explore how these problems might be avoided, 
We decided to implement another way to combining several flat XBW tables. 
In this way, consecutive graphs encoded as flat XBW tables are fused together on their ends, 
meaning that the dollar sign node of one graph is connected with the hash tag node of the next. 
This connection is not directly achieved within the data structure itself, 
but it is instead performed by the program working on the data structure, 
which in itself just contains several completely regular flat XBW tables with 
one hash tag and one dollar sign node each.

Implementing the fusing behaviour is not particularly difficult, 
as the only new occurrence to look out for is a possible spillover in which 
we leave one graph and move on to the next. In that event, the program working 
on the fused XBW data structure needs to automatically jump over the hash tag and dollar sign nodes, 
as internal hash tag and dollar sign nodes should not be accessible to the outside.

% TODO :: add more!

% TODO :: maybe we could add a picture of how two graphs are fused together on the ends?
% and show how much work would be involved in merging - that is, making the stuff prefix
% sorted after fusing - while all that can be avoided in simply letting it be that?

\chapter{Results}
% unsere Ergebnisse
%
% ====================================================================================================================================
%                                                                                                                              RESULTS
% ====================================================================================================================================

In this thesis, existing graph reference approaches have been compared 
and new algorithms have been implemented to help the community effort 
of starting to use reference graphs more widely.

\section{Formats for Genomic Graphs}
\label{sec:results_data_formats}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                  RESULTS: Formats for Genomic Graphs
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We tried out different formats in the course of this thesis, 
and noticed that indeed some do seem more appropriate than others 
for representing genomic graphs 
and in particular population graphs used as alignment references.

\subsection{FASTG Format}

Even though encoding genomic graphs is possible in the FASTG format, 
we noticed that it has some severe drawbacks.

Files in the FASTG format are unnecessarily big, which is mostly caused 
by a lot of intentional whitespaces but also by the decision to repeat the 
default interpretation of each FASTG-specific command just before the command. 
In the following FASTG example, the underlined 
text is just repeated and therefore unnecessary information:

...GCA\underline{TATGTCCTCTCTCC}[1:alt\pipe TATGTCCTCTCTCC\pipe GTC]GG...

On the other hand, when eliminating most of the whitespaces to reduce the file size, 
FASTG files which are already rather difficult to understand quickly 
become more and more obscure, such that they are no longer easily human-readable. 
However, that very human-readability is one of the main advantages of FASTA, 
and without it not much of a reason is left for using this particular family 
of formats altogether.

The FASTG format also is not very flexible. This can be seen by the fact that 
it only allows for three layers of variation within the entirety of the data. \\
The first layer considers global variation that is implemented through a mechanism in which the 
FASTA comments define which sequences can lead to which others. \\
The second layer is about local variation that is implemented through FASTG-specific constructs 
directly in the genomic data. \\
The third layer represents highly localized variation (such as snips) that can be nested 
inside of other FASTG-specific constructs from the second layer. \\
As not all of the constructs can be nested inside of each other 
and as the comment-based global and construct-based local 
variation are completely different approaches, this format 
seems unnecessarily complex.

Finally, the FASTG format as defined in 2012 with 
its many specialized constructs seems to be excessive for 
the needs of the Python test pipeline implemented in the course of this thesis. \\
Trying to implement the entirety of FASTG would in fact 
also be complicated by the fact that the format is open to amendments, 
such that solely implementing the existing standard would not be enough 
to be able to correctly work with all FASTG files which may be encountered. 
Even the specific bubble notation used within many FASTG files 
% which looks similar to ...ACGT[C,T]TAGT... 
does not actually occur in the standard written in 2012 \citep{specGFA1,specFASTG}.

\subsection{GFA Format}

Unfortunately, GFA has several drawbacks as well.

A crucial problem is that GFA has only shortly been worked on, 
and no definitive standard has been published. \\
Although several people have responded quite favourably to the original blog post 
outlining the format, many have also come up with improvements and outright criticism, 
making it less likely that the format in its current state 
is already suited for implementation \citep{knightGFA1}. \\
The lack of a unified standard also means that it is not even clear which version of it 
to base an implementation on.

In the context of this thesis it is also important to consider another drawback. 
According to Heng Li, the author of the format, GFA only aims to be an assembly format \citep{specGFA3}.
% "GFA is not limited to an assembly format. It can represent arbitrary relationship between sequences and is thus suitable for a population graph format in theory. However, I would not take applications on population graphs, such as graph mapping, as a killer application of GFA. There are many open questions on population graphs. I cannot design a format for unsolved questions. GFA aims to be an assembly format only, at least for now."
This means that it can represent a graph based on one particular read assembly. \\
It has not, however, been designed to represent population graphs---which are exactly 
the reference graphs for which it would be helpful to have a useful format.

Finally, this format is very broad and allows for a lot of complicated 
graph behaviour, especially considering its elaborate CIGAR strings. 
This seems to introduce complexity that is, at least for now, 
unnecessary for the purposes of creating a simple 
implementation of a graph reference based alignment tool.

% TODO :: completely overwork this, as apparently the standard came a lot further and now what we wrote here
% a year ago is not correct anymore... oops!

\subsection{Bubble Format}

Surprisingly, implementing an algorithm to open a file in the seemingly simple bubble format was actually 
not easier than implementing more advanced formats like FASTG, GFA and GML.

% TODO :: a bit more text please

\subsection{GML Format}

While using the GML format within the Graph Merging Library, 
it turned out that although it is a simple format for easy graphs, 
modifying graphs with it by hand is rather cumbersome. 
Especially deleting nodes on the main row is a quite elaborate process, 
in which the pointers of all paths leaving or entering the main row 
need to be updated. \\
This problem however could be mitigated by implementing a special text editor 
for the GML format, which would be able to handle common functionalities such as 
removing a node and inserting a node, while automatically updating the relevant pointers.

% TODO :: more

\subsection{FFX Format}

% TODO :: nicer introduction, and maybe also a bit more focus on FFX and what it can / cannot achieve (especially with fused data?)

We noticed that none of the considered formats currently offer inbuilt compression techniques. 
Changing this however would not be very difficult. Especially in the FFX format, 
which mainly contains a BWT and two bit-vectors for each data block, 
a rather simple compression method would be to save both the BWT and the bit-vectors 
using run-length encoding. 
% TODO :: insert figure of FFX with run-length encoding switched on
Such an approach would still keep the format simple enough to be implemented in various 
tools in the future, while offering a significant file size reduction. 
This reduction is especially strong when data is encoded that contains long linear sections 
interspersed with some short graphs, as both of bit-vectors will then 
contain very long runs. \\
It is of course also possible to create even more compressed versions 
of the different formats by not saving the contents of the files as ASCII text, 
but instead as bitwise data structures that can e.g. represent each of the nucleotides A, C, G and T with 
just two bits.

\section{Automated Tests}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                             RESULTS: Automated Tests
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In general, we can confirm how well the Graph Merging Library functions by checking 
that for a certain set of predefined inputs, the correct outputs are generated. 
For that purpose, we provided several kinds of automated tests for both the 
graph and XBW generation from an input, as well as the merging of several input graphs 
in different ways.

\subsection{Predefined Tests}

The basic approach for testing the validity of the GML algorithms is to run a set 
of predefined tests, for which the correct solutions are known. Doing so will 
show a list of tests and the test results, which can be one of four states. 
Figure~\ref{fig:evo_gml_test_buttons} shows how these predefined tests can be executed in the graphical user interface 
of the Graph Merging Library. \\
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{evo_gml_test_buttons.png}
\caption[GML Test Execution]{GML screenshot with row of three main action buttons. Clicking on the right-most button starts the predefined tests, while clicking on the button on its left starts the randomized tests.} \label{fig:evo_gml_test_buttons}
\end{figure}
The ideal result is a \textit{success}, in which the algorithmically generated result agrees with the 
expected result of the test. \\
Instead, a \textit{failure} can be reported if the result as generated by the GML algorithms 
differs from the expected result. \\
Finally, the label \textit{crash} is associated with a test which causes the 
entire program to crash completely, without generating any useful results whatsoever. 
Ideally, no input data should ever be able to cause this to happen, 
but if it should happen, then a crash would be reported to alarm the user to it. \\
The representations of all these result types in the GML interface can be seen in figure~\ref{fig:evo_gml_different_test_results}. 
\begin{figure}[!htb]
\centering
\includegraphics[width=0.95\textwidth]{evo_gml_different_test_results.png}
\caption[GML Test Results]{Different test results in the GUI of the Graph Merging Library.} \label{fig:evo_gml_different_test_results}
\end{figure}
A list of all predefined tests for the construction of XBW node tables can be found 
in section~\ref{sec:appendix_graph_const_tests} in the appendix. 
Section~\ref{sec:appendix_graph_merge_tests} contains the predefined tests for 
the merging of XBW node tables and flat XBW tables. 
Each of these lists also contains a smaller optional section at its end, 
which contains graph inputs which are intentionally invalid. These are used 
to ensure that GML reports an input as invalid when incorrect graphs are entered. 

\subsection{Randomized Tests}

The predefined tests are intended as a method for gouging how well the algorithms 
are functioning, but it could be argued that they might be hand-picked 
to only show positive outcomes. 
Therefore, a second mode of testing has been implemented 
in which random test data is created during the execution time of the program. 
However, generating such random test data automatically means 
that it is difficult to ensure that the input conforms 
to the core assumptions as formulated in section~\ref{sec:gml_core_assumptions}. 
It is in particular difficult to randomly generate test data for which 
no spillover in the splitting of nodes across several graphs which are 
supposed to be merged could possibly happen during the merging process. 
Opposed to that, it is straightforward to ensure that the input graphs 
contain no loops.\footnote{\,\,\,This can be achieved by starting out with a 
random sequence of characters as one path from the start to the end 
of the graph. 
Then new paths can be added one at a time to the emerging 
graph, with each new path from node $ u $ to node $ v $ only being 
allowed to be added if $ v $ could already be reached from $ u $ before 
the new path was added.} \\
It is therefore necessary for the program to ensure that the 
input can actually be worked on, and to give out a warning if not. 
That is why another possible test result with the label \textit{invalid input} can be reported. 
This occurs when the program determines that the input does not conform 
to the previously formulated core assumptions. 

\subsection{Test Analysis}

For the Graph Merging Library to be able to report whether a test is a success or 
a failure, the more robust graph merging from section~\ref{sec:direct_graph_merging} 
is used to generate the result that we 
wish to see. Then, a more advanced algorithm is used for the direct merging 
of XBW node tables or flat XBW tables. 
The resulting flat tables are then compared. 
% GAC|,1,T,3 and AC, on 150%, with tables on 300%
\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{evo_test_comparison.png}
\caption[Test analysis]{Test analysis by comparison of flat XBW table generated after robust merging of graphs and flat XBW table generated through advanced merging of several flat XBW tables.} \label{fig:evo_test_comparison}
\end{figure}
This approach is illustrated in figure~\ref{fig:evo_test_comparison}. \\
It is however entirely possible for the two tables to be different, but for the result to 
still be a success. That is, the flat XBW table generated through one method and the 
flat XBW table generated through another method can be different if the order in which BWT 
entries within a node are listed is different, as it does not make a difference in which order 
this listing is reported. Also, the tables can be different in case of nodes having been split 
by a method which are not entirely necessary to be split. 
This needs to be considered especially when prefix doubling is used for XBW node table merging, 
as the prefix doubling by design propagates node splits which are not strictly necessary, 
in order to more quickly build prefixes. \\
To be able to compare the results of different methods anyway, and call a test 
a success even if not the exact same flat table has been generated, but a nevertheless 
equally acceptable table has been constructed, we convert the tables in question to the 
primitive automatons described in section~\ref{sec:internal_graph_format}. \\
We further introduced the function \texttt{automatonsAreEquivalent}, which 
takes in two automatons to give out \texttt{true} if both are equivalent, 
and \texttt{false} if they are not. We here define two automatons to be 
equivalent if they generate the same language, even though they may differ 
in the amount and layout of nodes that they use to construct this language. \\
We can therefore check if both methods that are compared produce flat tables 
which correspond to automata generating the same languages, which means 
that graphs are generated which contain the exact same paths.

Even with the ability to compare the languages generated by automatons, 
the automated tests still have to be analysed carefully instead of 
simply being taken at face value. \\
Consider a merge test with several randomized inputs showing a mismatch 
between the expected result and the actual result. 
Usually, this should be reported as a failure because the actual result of the algorithm 
failed to be the expected result. \\
On the other hand, the same behaviour can also occur if the randomized input 
does not conform to the basic assumptions. If the program worked correctly, 
this would be recognized automatically and the input would be flagged as invalid. 
However, if we would report this as a failure then there would be a bug somewhere 
in the program, so that bug could just as well be in the part of the source code 
which detects whether inputs are invalid or not. \\
In general, it is not always entirely clear whether the program failed to provide the correct answer 
because the input was invalid, or because a bug prevented the generation of the correct output. 
The reason for this is that determining whether the input is invalid or not is part of that 
very same source code, and if a bug prevents it from correctly working, then this may be determined 
wrongly. In the opposite way, a bug might also prevent an actually invalid input from being 
flagged that way, resulting in a reported error which actually is no error at all. \\
Overall, this means that both tests which are reported as failures as well as tests which are reported as invalid input 
should be manually investigated more closely to 
better understand the underlying cause of the categorization. 
Having said this, in several thousand testing rounds the categorizations have held up completely, 
and the reported outcome was indeed correct. A more detailed description of the tests which have been performed 
and analysed can be found in the next section.

\subsection{Test Results}

Running the over 100 predefined tests for the XBW creation, node table merging 
and flat table merging in Mozilla Firefox under Microsoft Windows 7 on a 2.40 GHz machine with 4 GB RAM 
resulted in successes for each of them, 
with no failures or crashes being reported. 
As the tests have been picked to include various challenging situations 
in which complicated node splitting behaviours arise and in which 
special sorting mechanisms need to be used to ensure the correct outcomes, 
it can be seen that the algorithms appear to be working fine.

In addition to the predefined tests, we also executed 4,000 tests with randomized data, 
testing the merging results of the flat XBW table merging algorithm. \\
280 of these randomized tests were reported to contain invalid input, 
while the remaining 3720 tests were all reported as successes, 
either generating the exact expected flat XBW tables, or at least 
generating XBW tables which corresponded to graphs which contained exactly the 
correct paths.

% TODO :: add information about node table merging test runs

It should of course be noted here that no amount of tests can ever 
truly prove the Graph Merging Library to truly be working correctly, 
as any amount of tests performed is necessarily finite, while there are 
infinitely many inputs that could theoretically be specified, 
such that it will never be possible to check every single input that there is.

\section{Merging XBWs}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                                RESULTS: Merging XBWs
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using the Graph Merging Library, we have explored several ways to merge XBWs. 
As could be expected, merging two node tables is a much simpler process than merging two flat tables, 
as a node table is much less compressed, and working on it is simpler in general. 
The node table not being as compressed as a flat table though means 
that it would not be practical in a real-life scenario to merge these kinds of tables.

However, even when choosing to merge two flat XBW tables, 
the merge result can become unnecessarily large.

% TODO :: insert picture - a graph, another, and then merged soooo big as the thing is split and split and split

\section{Fusing XBWs Instead of Merging Them}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                         RESULTS: Fusing XBWs Instead of Merging Them
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

When merging flat XBW tables together, it is common that node splitting 
will result in a merged graph with a size larger than the total size of the 
original graphs. 
Fusing flat XBW tables together omits the need for this increased file size, 
which not only makes it easier to store the data, but also means that working on 
it is quicker as less data means that fewer operations are necessary to construct 
distinct prefixes, to traverse all possible paths, and so one.

However, fusing flat XBW tables also has downsides. 
One of them is related to indexing. 
When working with a merged graph, every column in the graph has a clear integer index, 
so that locations within the entire graph can simply be referred to with a single integer value. 
With a fused XBW however, in addition to specifying a column within the table, 
it also needs to specified which table is addressed, necessitating pairs of integers. 
If particularly large list of locations needs to be created, this difference of two integers instead of one 
could become problematic. \\
Also, it is algorithmically more complex to work with fused graphs, as every 
algorithm working on the graph needs to be prepared to handle spillover scenarios, 
in which the end of one of the fused graphs is reached and the execution needs to continue 
within the next graph. \\
These spillover scenarios might also severely limit the otherwise seemingly 
great possibilities for splitting the computation of fused graphs across different 
cores in a distributed system. 
If each processor core is supposed to work on exactly one graph, 
but spillovers are happening frequently, then some cores would stop having work 
to do as no further work would be going on within their data block, 
while others would have to do twice as much work by caring for their 
own work before working on the activity that spilled over from the other data block. \\
Finally, setting up such a system in the first place might be difficult to do, 
as a fused file would most likely contain large areas of linear data, 
fused together with short blocks of graph data. Therefore, assigning one 
data block to one core and letting it perform work in this straightforward fashion 
would lead some cores to have to do much more work than the others, ultimately 
leading to many idle cores waiting for a few to finish which were given much more work 
to do in the first place. This is obviously not desirable in a multi-core system, 
in which ideally all the processor cores should be working for the same time 
and then be done together in the quickest time possible.

However, at least some of these problems could be overcome fairly easily. 
Spillover scenarios as an example, which with a naive 
implementation would possibly lead to some cores having no work to do 
after a spillover happened while other cores would have to do twice as much, 
could be avoided by giving the cores access not only to their own data blocks, 
but also to the data in the surrounding blocks. Then each core could handle 
and complete its own spillovers, without requesting assistance from its neighbours.

\chapter{Conclusions}
% AKA: summarize our data and compare with methodology to see if we fulfilled what we set out to do
%
% ====================================================================================================================================
%                                                                                                                          CONCLUSIONS
% ====================================================================================================================================

Aligning reads from human DNA to graph references rather than to string reference sequences 
still offers many interesting problems. 
However, the first steps on the way to using reference graphs in real-world applications have been made.

\section{Discussion}

Despite a lot of work already having been done to explore 
ways of using genomic graphs within the field of human DNA analysis, 
using reference graphs rather than reference strings still 
is a difficult proposition and only few tools exist that incorporate 
such methods. 
It could therefore be argued that the known advantages of using reference 
graphs do not outweigh the tremendous difficulties that have to 
be overcome in order to be able to use them on a large scale. \\
After all, the reason why reference graphs are proposed to be used 
rather than reference strings is to improve alignment results 
and to simplify the calling of known variants. 
But at least in the beginning when only small local graphs are incorporated 
into largely linear references, the improvements are likely to be very slim. 
Likewise, it could be questioned whether simplifying the calling of variants really 
justifies complicating matters so much in general by the introduction of 
these new graph structures.

However, we would argue that in the long run, reference graphs will come to be used. 
With every new individual DNA that is constructed, more insight is gained into 
variations which could be quite common among populations. 
Discarding all of that data in favour of a simpler but ultimately flawed approach 
will not work forever, as over time better alignment results will come to be expected. \\
Therefore, it seems inevitable to work on reference graphs eventually, 
and it seems only logical to start that work as soon as possible, rather than investing 
more time and resources into improving current alignment tools which are likely to be drastically 
modified to incorporate reference graphs in the future anyway.

\section{Future Work}
% AKA: Possibilities for SOMEONE ELSE to make it even better

There still remains a lot of future work to be undertaken to actually use 
the already existing and the here newly proposed algorithms in real-world 
tools of the analysis pipeline. 
As various different tools are in use for specific problems, no one implementation 
is likely to achieve this. Instead, the community at large needs to start 
incorporating the ideas of using graph data rather than sequential data into 
more and more of the existing tools.

\bibliography{thesisreferences}

\appendix
\renewcommand{\chaptername}{Appendix}
\chapter{Appendix}
%
% ====================================================================================================================================
%                                                                                                                             APPENDIX
% ====================================================================================================================================

\section{Predefined Graph Construction Tests}
\label{sec:appendix_graph_const_tests}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                        APPENDIX: Predefined Graph Construction Tests
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% TODO :: keep these lists up to date!

The following tests are run to ensure that the Graph Merging Library does not throw errors 
while constructing flat XBW tables for input graphs:

\begin{lstlisting}
ACEG
ACEG|,1,,3;,2,TB,4
GACGTACCTG|,2,T,4;,3,,5;,6,,10;,6,,8
GACGTACCTG|,2,,4
ACGTAGATTTC|,4,,7
TAATACGCGGGTC|,8,,9
GACGTACTG|,5,C,8;,2,G,5;,5,GT,9
GCTGGCGAGCAG|,3,T,6;,5,TC,12;,5,,7
GCTGGCGAGCAG|,3,T,5;,5,GCTC,12
GACGTACCTG|a,2,TAT,6;,a:0,C,a:2
GACCTAATG|,5,C,8;,2,G,5
ATTCACCACCAACCCGACATA|,4,CACG,9;,9,TCG,12
CCACGCGCCATGGC|,1,CTG,14
GCTGGCGAGCAG|,3,T,5;,5,,7
GCTGGCGAATGTGGCAAGAATGTGGCAAGAGCAG|,3,T,5;,5,,7;,4,CACG,9;,9,TCG,12
GCCGT|,3,,5
ATCT|,2,A,3
ATCGAT|,2,,5;,4,CG,6
TGATGAG|,6,,7
GTTAATGTGGCAAGT
TCCCTGT
CTACCAGGTGCTGTTATTCCAC|,16,A,22
CTATGTTATTCCA|,8,A,13
TATA|,2,A,4
CCA|,1,A,2
TTCGCAAGA
CCTGCGTGTGGT|,3,,6
CCGAGCTTATCGCAA
ATGCGAGTTCGGGAC|,9,CCCG,14
GCAGGGAGCTCCAGCCGTTAG|,4,CC,13;,1,A,18;,17,,19
ATTGGAGAAGTCACGCTTGAC|,6,CTA,14
CACTC
TGCAAGTATGGCGCTT|,2,,8
AACCAT|,4,GGG,6
AACCAT|,4,GGG,6
AGAGCACGCCGGT
AGAGCACGCCGGT
CGGCAATAGACGCTGTCCAATGC|,15,,18
AGAGCAT
ACTGGCATGATTTATCCCTTGG|,11,,12
GGGCCCAGGGCGATCGACGTTC|,7,,19
CCCAGCC|,4,,6
CAGCC|,2,,4
CACT
CACT
TC|,1,C,2
ATCAACTTTCC|,6,CA,7
GCGGTGAAGAGAAA|,2,CGG,7
CCACGCTAAGTTATCGTGT
ATCCAATCGTAAT|,7,T,8
TAGCTTGGAC|,2,AG,9
GACAAACACAAACATCACCCTGT
CGTCGCC|,6,T,7
AAAGTAT
TACTTGTC
GCAGAATTCCGCAGGAAAGC
CGCGACAGTGGCCAATCT
GGTACATGGGAT
TCTGGTACGCTG|,1,,11
GGGAACAGATGTCTGTGATCC|,13,CAT,14
CGAGTTACGTGGCCGCCTCAT
TGTCTGTTACAGATTGC
CGAAGCCTA
CGAAAGAGTGTGTCTAGGC|,11,TC,19
TTTAGGTTAGTACCAC|,8,CTG,11
TACTTCCCCAGGACGGGACGCTA
AAAGGAGGT|,7,,8;,2,A,6
GGTGGCCGAGTGC|,10,CTGC,12
CGAATACCGTACTGAA|,12,AAAA,15;,14,GGTC,15
TTCCAATGGTGAGTCTC
AAGAT|,2,CTC,3;,1,C,4
AACAAATT|,4,TCCT,8
AAAAGAAGCTGAT|,4,,6
ACGTGCCCACCCG|,2,A,8
TAGGCGCGGT
TTTGCATCATATC
TTGTAG|,5,A,6
CTATATAGCGGC|,4,CTA,7
ATTAATACG|,2,G,5
CGGCCT|,1,A,6;,2,TT,4
CATCTTTC|,5,CA,7
AGGCAGCTATCGACCATCTTGCG|,22,,23;,2,TT,9;,9,C,10;,22,,23
AAGAGTCCAGAG|,7,,11;,5,TATA,9
GCCACGACACCCTCAAGCT|,17,CA,18
TTAAA|,4,GAT,5
TACCAGGGCTTTTTTACTGGCT|,8,,19;,20,GCT,21
TGAAGCGCATTCT|,9,,11;,3,,11
CGGATAGCACTCTA|,9,TT,11
TCCCTGGGC
CAAGGTATTGTTAA
CTCGGCTCT|,1,AAGG,2;,4,,6;,8,TG,9
AGGGAGCCTTAACATTTTCG|,13,T,19;,11,AGCG,15;,10,A,12
TCAGGGCGAGC|,6,G,9
GGAGCCAGGCTTGCCC|,7,,11
TGGCCCTCCCCCTACAT
AGGCCATTGATGAAAA|,7,AT,9;,5,CCT,12
GTATGCT|,6,TACC,7
ATTATCGATCA|,3,TCT,7;,10,CGCT,11;,8,TTCA,11
GTAACCTTGAGGAAGG
GTCGAATGATTCGCC|,8,ATT,15
CTATA
GGAAGG
\end{lstlisting}

The following tests are run to ensure that GML correctly recognizes whether a dataset is an invalid input:

\begin{lstlisting}
GACGTACCTG|,2,T,12;,3,,5;,6,,10;,6,,8
GACGTACCTG|,4,T,2;,3,,5;,6,,10;,6,,8
GACGTACCTG|,2,T,4;,a:3,,5;,6,,10;,6,,8
ATCTCAG|,1,GG,4;a,2,C,6;,a:1,G,7
\end{lstlisting}

\section{Predefined Graph Merge Tests}
\label{sec:appendix_graph_merge_tests}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                               APPENDIX: Predefined Graph Merge Tests
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% TODO :: keep these lists up to date!

The following tests are run to ensure that the Graph Merging Library does not throw errors 
while merging XBW tables for input graphs:

\begin{lstlisting}
ACEG and BDFK
ACEG|,1,,3 and BDFK
ACEG|,1,,3;,2,TB,4 and BDFK|,2,,4
C and ACTG|,2,,4
C and ACATG
GACGT|,2,T,4;,3,,5 and ACCTG|,1,,5;,1,,3
C and ACTG|,2,T,4
GACGT|,2,C,4;,2,T,4;,3,,5 and ACCTG|,1,,5;,1,,3
GACGT|,2,T,4;,1,C,4;,3,,5 and ACCTG|,1,,5;,1,,3
TAT and C
TT|,1,A,2 and C
TCGTGCGAGG|,1,ACAA,10 and C
TAATACGCGGGTC|,8,,9 and ACCTG|,1,,5;,1,,3
AAGTTTCTTTCGTGCGAGGCCGT|,10,ACAA,19;,16,CGT,20 and ACCTG
ATAGTCAATTGACTGCCGACG and CGGGGTAAAAAAGCGCC
GCCG and GCGC
GCCG and BDFK
GG and CGG
ATCT|,2,A,3 and CC
ATCGAT|,2,,5;,4,CG,6 and C
TGATGAG|,6,,7 and GCCAGGTCAGCCTAGTCCCTG
GTTAATGTGGCAAGT and CTCGCGACGACTAAAGCTGGCC
TCCCTGT and CTCAGCAGAGGCCCAGGCAAA|,13,TA,15
CTACCAGGTGCTGTTATTCCAC|,16,A,22 and CATCGATTT|,3,CGAT,9
CTATGTTATTCCA|,8,A,13 and C
TATA|,2,A,4 and C
CCA|,1,A,2 and CA
TTCGCAAGA and TCCCCAAG
CCTGCGTGTGGT|,3,,6 and CTCTTA|,3,TCG,5
CCGAGCTTATCGCAA and GAGAGAGCAAC
ATGCGAGTTCGGGAC|,9,CCCG,14 and TCCAA
GCAGGGAGCTCCAGCCGTTAG|,4,CC,13;,1,A,18;,17,,19 and CGGAT
ATTGGAGAAGTCACGCTTGAC|,6,CTA,14 and GTTAAAACAC|,6,TC,10
CACTC and GGGCAGTACGTGG|,9,,11;,7,CGCG,8
TGCAAGTATGGCGCTT|,2,,8 and GTGTAATTATGAAC|,13,GATG,14
AACCAT|,4,GGG,6 and GTGCAGCTTTTGG
AACCAT|,4,GGG,6 and TTAAGGTTATGGACCCCCC|,4,GGAT,8;,6,C,12
AGAGCACGCCGGT and TTAAGGTTATGGACCCCCC|,4,GGAT,8;,6,C,12
AGAGCACGCCGGT and CGTAAATAGAG|,7,G,9
CGGCAATAGACGCTGTCCAATGC|,15,,18 and CGTAAATAGAG|,7,G,9
AGAGCAT and AGTGAACACAC
ACTGGCATGATTTATCCCTTGG|,11,,12 and TTTGTTTGACACGCTGC|,16,,17;,11,AGGG,14
GGGCCCAGGGCGATCGACGTTC|,7,,19 and TTTGTTTGACACGCTGC|,16,,17;,11,AGGG,14
CACT and GCGTACG|,5,,7;,1,C,5
CACT and GCGTACG|,4,,7;,1,C,5
TC|,1,C,2 and TC
ATCAACTTTCC|,6,CA,7 and TACCGAACGCCGGTGTGATAA|,6,,8
GCGGTGAAGAGAAA|,2,CGG,7 and GACTC
CCACGCTAAGTTATCGTGT and GCTTTATACGTAAT|,10,CC,13
ATCCAATCGTAAT|,7,T,8 and TCGAATTATCACGATAT|,13,AAA,17;,12,CG,15;,7,,15;,7,GAG,16
TAGCTTGGAC|,2,AG,9 and GTGATGT|,5,,7
GACAAACACAAACATCACCCTGT and TGAAATAGT|,4,T,6
CGTCGCC|,6,T,7 and TCAATGTATAGTCGTGTCTGAAAT|,8,TGCA,23
AAAGTAT and AAAGCGAGCAAAAGATTGACGAA
TACTTGTC and CTCAACT|,6,,7
GCAGAATTCCGCAGGAAAGC and CATGGCGTCTAGGA|,10,GC,12
CGCGACAGTGGCCAATCT and TCGTCTGATGATCTTTGCTCTC|,19,,20
GGTACATGGGAT and ATGCGCTCGAAGCACGAAGG|,6,AT,15
TCTGGTACGCTG|,1,,11 and ATGCAAAAGTTTAAAGTTGTCCGT|,3,CGCT,18
GGGAACAGATGTCTGTGATCC|,13,CAT,14 and TGAAAAAAACTCAGCGGGGAGT
CGAGTTACGTGGCCGCCTCAT and CCGTA|,3,TGT,5
TGTCTGTTACAGATTGC and AGACATCCATC|,4,,11;,10,TC,11
CGAAGCCTA and TTGTGAATATCG|,6,CTG,9
CGAAAGAGTGTGTCTAGGC|,11,TC,19 and AGAGCT
TTTAGGTTAGTACCAC|,8,CTG,11 and CATAGCTTAGCC|,11,,12
TACTTCCCCAGGACGGGACGCTA and GGTCAG
AAAGGAGGT|,7,,8;,2,A,6 and GAGTTATGACG
GGTGGCCGAGTGC|,10,CTGC,12 and CATTTGTGGCGCGCTTGATCTCTG|,10,,20
CGAATACCGTACTGAA|,12,AAAA,15;,14,GGTC,15 and ACAGCGAGAGCAGACG|,14,TC,15;,8,,16
TTCCAATGGTGAGTCTC and ATAAAG|,3,AG,5
AAGAT|,2,CTC,3;,1,C,4 and ACGCTCCTTTGTGG
AACAAATT|,4,TCCT,8 and GAGCGGAGAATCATACGGGC|,16,GAG,20;,1,CGC,17
AAAAGAAGCTGAT|,4,,6 and CTTGAA
ACGTGCCCACCCG|,2,A,8 and CCCCAACTGGGAATTC|,13,TCC,16;,4,TTT,14
TAGGCGCGGT and CTGGGACGG|,4,GTG,7;,2,C,6;,4,TGA,7;,6,C,9
TTTGCATCATATC and GAAACATTTATGACATTA|,4,,12
TTGTAG|,5,A,6 and AACGCGT|,3,,6
CTATATAGCGGC|,4,CTA,7 and ACGCATATTGGTTC
ATTAATACG|,2,G,5 and TTTTTATGGTCCGACTAA
CGGCCT|,1,A,6;,2,TT,4 and TTAATCT
CATCTTTC|,5,CA,7 and GTTGGTGGCCATCGG|,11,,15;,4,GG,14
AGGCAGCTATCGACCATCTTGCG|,22,,23;,2,TT,9;,9,C,10;,22,,23 and AGCTGGGAAAC
AAGAGTCCAGAG|,7,,11;,5,TATA,9 and AGTGGCTGATTGGT
GCCACGACACCCTCAAGCT|,17,CA,18 and TCAATTGCCGGTA
TTAAA|,4,GAT,5 and GAACTCGGGGC|,3,TTGT,5
TACCAGGGCTTTTTTACTGGCT|,8,,19;,20,GCT,21 and CCGGCGGATTGATAGTGC
TGAAGCGCATTCT|,9,,11;,3,,11 and GGATGCGAGCAGTACGTCTCCTTC
CGGATAGCACTCTA|,9,TT,11 and ATGTTGGGCCCTAA|,7,A,12;,8,T,11
TCCCTGGGC and ACGATTCCCTAACCACCCGT|,17,ATG,19;,7,TT,16
CAAGGTATTGTTAA and AGTCTAAGGCTTCGTGTTGGATC|,9,GTA,22
CTCGGCTCT|,1,AAGG,2;,4,,6;,8,TG,9 and AAAGTCTTCAATGT
AGGGAGCCTTAACATTTTCG|,13,T,19;,11,AGCG,15;,10,A,12 and CATGTTGGTTGT
TCAGGGCGAGC|,6,G,9 and GACAACCCAGATTTTCCCGCATT|,10,,18;,15,TAA,17
GGAGCCAGGCTTGCCC|,7,,11 and ACAGGATAGGTCATTGAGTGGGG|,11,CAAC,16
TGGCCCTCCCCCTACAT and TAGGCCGAAGTTTCTCTATCTTG|,13,CAAA,18;,16,AAC,19
AGGCCATTGATGAAAA|,7,AT,9;,5,CCT,12 and GACTG|,2,,3
GTATGCT|,6,TACC,7 and GGACCTACGGCGGTTTAAAA|,8,A,14;,2,GCGA,12
ATTATCGATCA|,3,TCT,7;,10,CGCT,11;,8,TTCA,11 and TGACAAGGTGCTCAGGTGAAA
GTAACCTTGAGGAAGG and GGGATTCAGTAGTTC
GGAAGG and GG
AATTCGGTAATACCGAAG|,15,AT,17 and TAAGATAAGGCAAA|,7,TA,11
CAGAGTATGCTCCGAG and CGATTACGCAT|,8,,10;,3,TA,9
GACGCCGACAAGAGACAAG|,2,CCA,11;,17,TC,18 and AGGTAGT
GTATAAATACAAGCTAAG|,17,CCA,18 and GATGA|,3,TCTC,4
CTAGA|,2,CC,3 and AAGATGAAAGGACCCCCA|,16,,18
GGGGAAAGCAAGATATCAGCCGT and CAAAAAGA|,4,,5;,3,C,5
ATGATG|,4,TC,6 and AAATGAGAT|,7,TGC,9;,5,AG,8
GCCAGA|,3,TAC,5 and CAACTCCAGGTTCCCTTATTG|,11,GTCA,13
GATATGGCGT|,3,A,9;,7,GTCA,10 and AAGACTTATACGC|,11,,13;,3,G,10
GCGGATATTTGGCGGCT|,6,A,14;,12,GTCA,15 and AAGGAAGAAGACTTATACGCC|,18,,20;,10,G,17
TGGAAGCACACCC and TAGGGCGGAAGGG|,5,A,11;,5,TCCT,11;,8,GCCC,12
GTGCCCTAGACCTCCTTAT|,5,,10;,13,CCA,14;,7,GT,11;,3,G,8;,12,TGGG,18 and GTGCA
TGTACTGGCTCCTATAT and TAACACATTAGTCAACCACA|,13,,16;,5,,12
CGACAG|,5,TTCC,6;,3,T,5 and GACCCACGCAAGAAACCGTGA|,10,TAAC,19
CGTTTGCATGAATTTGTTT and AACGTACCATTAAAATAGCGA|,6,,9;,4,TG,20
TACGATCGGA|,2,ATCG,6 and TAAGTTA|,2,CTG,3
GAGTATAAGCCA|,9,,10 and GGCGCGGTCAGTT|,12,GC,13;,2,GAT,6
AACCA|,4,CTTT,5;,4,CA,5;,4,ACG,5 and AGGTTATAGAT|,7,CAT,9;,2,,11
TCCGCTT|,2,TTTG,6;,4,ACT,6;,3,GGTG,6;,5,AACG,7;,2,ATC,5;,4,,5;,3,AA,6;,3,GGA,7 and TCGACATTGCTGCAGGTT|,14,CT,18
CCCAGCC|,4,,6 and TT|,1,AGG,2
CAGCC|,2,,4 and TT|,1,AGG,2
TTCCGCGGCTCCATG|,14,CGTA,15 and ACCGTCCTCA|,8,CCGT,9;,8,C,10;,4,,10
ACTCTGAATGCTGTACACTCA|,5,,19;,13,C,14;,17,AGA,21 and GTTTGGGGA|,6,,9
ACCTG|,3,AAC,5;,1,C,2;,1,C,2 and ACGATGAAACGGATTGTA
TACACG|,4,AAT,6;,1,CCGA,5 and TGGTGTATTATTGCCCTTGGACGC|,20,GA,22;,5,G,17;,8,AATA,10
TGTAGATGAGCATACCCCGA|,4,ACGA,20;,11,,19;,15,TT,19;,6,,11;,13,AA,20 and AAATATCTTTGTTG
CTCATACGAGCGAA|,12,CG,13 and GTGCCCGGGTGCA|,1,TTTC,11
TTTTAATAGTCACCGATCCGG|,17,CGTT,21;,19,TTT,20 and TCGGCATT|,4,ACGA,7;,6,GGA,8;,3,TGA,8
CCCAATGACAATG|,7,TAGA,10;,5,ACCC,10;,4,C,11;,4,,8;,8,C,11;,7,AGGT,9 and GTGTC
GCTAGTTACAACT and GCAAGCGAA|,3,TCG,4;,5,T,8
CAGTTTACAACTGCAGCCGTTA|,7,,12 and TCGCCCGTAACTCAACGTTTAA|,16,CTGG,17;,19,A,20;,19,GC,20
AACCCT|,1,ATCA,3;,2,C,4;,3,,5;,4,TCAT,6 and CCTTCGTACTAGCACAAACT|,5,GGTT,11
TTGCTAACCAGC|,4,TC,5 and GGTGTTTGGGGCCCATACAAAT|,15,A,18;,1,GA,11;,5,CCGC,14;,10,,19;,5,,13
\end{lstlisting}

The following tests are run to ensure that GML correctly recognizes whether a dataset 
consisting of two graphs which are supposed to be merged contains an invalid input:

\begin{lstlisting}
CTATA and AAATATG|,3,AGTT,6
GTCGAATGATTCGCC|,8,ATT,15 and GCTTGTGCCTAA|,8,GT,12;,7,TGT,10;,2,A,12
TGGCC|,1,TA,2 and ACGCAGCTGCGAGAATA|,1,ACCC,5
GTAGCAAAGCGGTTTGGGATTG and GGTACCTATAGGGAA|,2,C,5
GAGTGGCCAAGTAGCCGGGTCACA|,23,GTC,24 and CGTACTG|,3,GAGC,7
GGTGGGATGGACTCTGTTGTG|,12,CACA,18 and CTGACTGTCAGGAGCTGA|,3,,9
GGTACGCGTTATCGT and CTGTCTATCTCTAACCC|,12,,16;,2,T,6;,11,GA,12
GTAAC and TTTCACGCGT|,5,C,10;,5,CTC,6;,1,CG,3;,8,A,9
GTAATGACGTAGCCC and ATCATCGCA|,3,CT,8
GGGTTAGCTGTCTAAG|,6,C,9 and ATGATGATA|,5,ATAT,6;,1,TA,2
GTAGAGAATGGACTACTG and TGTGTGTTC|,2,ACG,4
ACGCAAGATGTATC|,3,GGAC,9;,10,G,11 and CCCCT|,2,CTT,5
TCGTCGTGAG|,5,G,6 and GAACCCGGAATCTTA|,2,T,15
ACTCGTGAAG and GGGCA|,1,CTCA,5
ATACGTAATCGGTGGCTCGA|,14,TCG,20 and AGATCGAGCTG|,4,,10;,8,,10;,2,CTA,7
ATGTACACTTCTGAGTTGTCC and GCGTTGCG|,1,G,4
CGGAGACTATCCTATCC|,9,CAG,15;,11,,16 and CAGTCATCGATTAAACCACTCTG|,1,GG,5
TGCTGCTTTGGAAAT|,1,ATTA,14 and GTCCGTAGCAA|,5,GG,7;,1,,7
GCTCTCATGCAGCCCAA and TTTCAGCCTAACAGTACACGGGT|,1,GA,19
CGCCC|,4,GAC,5;,4,,5;,3,ATG,4 and CAAGCTG|,1,GTTA,5;,5,GA,6
ACTCCTACTCCAACTA|,9,GTT,13 and ACTACC|,4,ATG,6
AACTAGGCGCATGTC|,3,CAG,8;,3,GTT,15 and CTTGGC|,3,AGA,6;,2,T,4;,2,CTAT,4;,1,AT,2
\end{lstlisting}

\section{Resources and Licensing}
%
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%                                                                                                    APPENDIX: Resources and Licensing
% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Publicly available genomic datasets as well as automatically generated data were used 
to implement the new graph reference approaches and to analyse the results of the implementations.

Recently, there has been a push towards implementing more open source scripts 
and programs within the scope of bioinformatics \citep{MANIFESTO}. \\
All of the software developed while working on this thesis 
is open source and distributed under the terms of 
the Creative Commons Attribution Non-Commercial License 
(\url{http://creativecommons.org/licenses/by-nc/3.0/}), 
% TODO :: check (ask someone?) if this is the correct license to be used here
which permits non-commercial re-use, distribution, and reproduction in any medium, 
provided the original work is properly cited. 
For commercial re-use, please contact \url{moyaccercchi@hotmail.de}. \\
The source codes together with the newest version of the thesis itself can be accessed at \url{http://tomschiller.de/graphalign}.
% 
% TODO :: add stuff to the appendix!

%
%
% don't let any reference remain unused!
% \phantom{
% References that are so far not referred to anywhere else in the text:
% }
%
%

\end{document}

% TODO :: check if all our lists are in the following format
%
% ullet list
% Hér á eftir er dæmi um upptalningarlista. Listinn má vera þéttari, 
% þ.e. að aðeins sé 0 pt bil á milli atriða, en þó skal hafa 12 pt bil á undan fyrsta atriði listans.
% \begin{itemize}
%  \item Númer 1
%  \item Númer 2
%  \item Númer 3
% \end{itemize}
% Ef fyrsta lína eftir upptalningu er framhald sömu málsgreinar og fyrir ofan 
% skal ekki setja inn viðbótarloftun (eða ef inndráttur er notaður, 
% skal ekki nota inndrátt á slíkri línu).

% TODO :: check if all our figures and tables are in the following format
% 
% \chapter{Figures and tables}
% { \color{red} Þessi kafli sýnir dæmi um notkun mynda, taflna og vísun í þær. }
% 
% \section{Figures}
% { \color{red} Myndatexti skal staðsetja undir myndum og skrifast með skáletri.
% Setja skal auða aukalínu fyrir ofan myndir.
% \begin{figure}[!htb]
% \centering
% \includegraphics[width=0.47\textwidth]{evo_example_pic.png}
% \caption[Dæmi um myndatexta (fyrir neðan mynd).]{Dæmi um myndatexta (fyrir neðan mynd)} \label{fig:Example Pic}
% \end{figure}
% Mikilvægt er að skilgreina myndir með ,,paragraph format”: “keep with next” til að rjúfa 
% ekki tengsl á milli myndar og myndtexta. Mynd má vera miðjuð og skal þá einnig miðja myndartextann. 
% Letur innan myndar skal vera í steinskrift (sans serif), t.d. Verdana, og ekki minna en 10 pt. 
% Tryggið að letur, tákn og línur sjáist skýrt eftir útprentun.
% 
% Hægt er að láta númera og merkja myndir sjálfvirkt með því 
% að gera Insert – Reference – Caption – Mynd eða Tafla. 
% Varist að velja hyperlink. 
% 
% Vísa má í mynd með því að velja Insert – Reference – Cross-Reference – Mynd eða Tafla. 
% Varist að velja hyperlink og veljið að eins Label og Number. 
% T.d. sjá þessa tilvísun í Mynd 3.1 sem dæmi. }
% 
% \section{Tables}
% { \color{red} Einnig má númera töflur sjálfkrafa svipað og myndir. 
% Nota skal skáletur í töflutitil. Textinn skal standa fyrir ofan töflu og fylgja töflunni. 
% Ekki nota tvöfalt línubil eða hafa space before í töflum. 
% Meginreglan við töflugerð er að hafa þær einfaldar og eins fá strik og mögulegt er. 
% Tafla má vera miðjuð á blaðsíðu og skal þá láta töflutitil byrja við vinstri brún töflu.
% 
% \begin{table}[htb]
% \centering
% \caption{Dæmi um töflutexta (fyrir ofan töflu).}
%      \sffamily \begin{tabularx}{1.0\textwidth}{ p{5cm}  p{5cm}  p{5cm} }
%     \hline
%    \textbf{Taflan} \hfill & \textbf{Er} \hfill & \textbf{Eins} \\ \hline
%     Og & Hún & gæti\\
%     Litið & Út & í ritgerð\\ \hline
%     \end{tabularx} \normalfont
% \label{table:Emissivity}
% \end{table}
% 
% Almennt skal ekki nota loftun fyrir neðan texta í töflu, og stylla loftun fyrir neðan á 0 pt.
% Mikilvægt er að skilgreina töflutexta með ,,format paragraph: keep with next og keep lines together” til að 
% rjúfa ekki tengsl á milli töflutexta og töflu. 
% Ef tafla er mjög löng má kljúfa hana á milli blaðsíðna og þá verður 
% að setja (\textit{Framhald}) í aukalínu beint fyrir neðan töfluna, 
% hægri stillt við hægri brún töflu.
% 
% Dæmi um sjálfvirka tilvísun í töflu, bara nota Label and Number, 
% ekki nota hyperlink eða caption text. T.d. Tafla 3.1 sýnir dæmi um töflu. }
